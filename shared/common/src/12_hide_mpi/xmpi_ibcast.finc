!{\src2tex{textfont=tt}}
!!****f* ABINIT/xmpi_ibcast
!! NAME
!!  xmpi_ibcast
!!
!! FUNCTION
!!  This module contains functions that calls MPI routine MPI_IBCAST,
!!  to broadcast data from one processor to other procs inside a communicator, 
!!  if we compile the code using the MPI CPP flags.
!!  xmpi_ibcast is the generic function.
!!
!! COPYRIGHT
!!  Copyright (C) 2001-2022 ABINIT group (MG)
!!  This file is distributed under the terms of the
!!  GNU General Public License, see ~ABINIT/COPYING
!!  or http://www.gnu.org/copyleft/gpl.txt .
!!
!! SOURCE
!!***

!!****f* ABINIT/xmpi_ibcast_int1d
!! NAME
!!  xmpi_ibcast_int1d
!!
!! FUNCTION
!!  Sends data from one processor to others inside comm without blocking
!!  Target: integer one-dimensional arrays.
!!
!! INPUTS
!!  root: rank of broadcast root (integer)
!!  comm: MPI communicator
!!
!! OUTPUT
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_ibcast_int1d(xval, root, comm, request, ierr)

!Arguments-------------------------
 integer ABI_ASYNC, intent(inout) :: xval(:)
 integer, intent(in) :: root, comm
 integer, intent(out) :: request, ierr

! *************************************************************************

 ierr=0
#ifdef HAVE_MPI_IBCAST
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_IBCAST(xval, product(shape(xval)), MPI_INTEGER, root, comm, request, ierr)
   xmpi_count_requests = xmpi_count_requests + 1
   return
 end if
#endif

 ! Call the blocking version and return null request.
 call xmpi_bcast(xval, root, comm, ierr)
 request = xmpi_request_null

end subroutine xmpi_ibcast_int1d
!!***

!!****f* ABINIT/xmpi_ibcast_int4d
!! NAME
!!  xmpi_ibcast_int4d
!!
!! FUNCTION
!!  Sends data from one processor to others inside comm without blocking
!!  Target: integer one-dimensional arrays.
!!
!! INPUTS
!!  root: rank of broadcast root (integer)
!!  comm: MPI communicator
!!
!! OUTPUT
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_ibcast_int4d(xval, root, comm, request, ierr)

!Arguments-------------------------
 integer ABI_ASYNC, intent(inout) :: xval(:,:,:,:)
 integer, intent(in) :: root, comm
 integer, intent(out) :: request, ierr

! *************************************************************************

 ierr=0
#ifdef HAVE_MPI_IBCAST
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_IBCAST(xval, product(shape(xval)), MPI_INTEGER, root, comm, request, ierr)
   xmpi_count_requests = xmpi_count_requests + 1
   return
 end if
#endif

 ! Call the blocking version and return null request.
 call xmpi_bcast(xval, root, comm, ierr)
 request = xmpi_request_null

end subroutine xmpi_ibcast_int4d
!!***

!!****f* ABINIT/xmpi_ibcast_dp1d
!! NAME
!!  xmpi_ibcast_int1d
!!
!! FUNCTION
!!  Sends data from one processor to others inside comm without blocking
!!  Target: double one-dimensional dp arrays.
!!
!! INPUTS
!!  root: rank of broadcast root (integer)
!!  comm: MPI communicator
!!
!! OUTPUT
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_ibcast_dp1d(xval, root, comm, request, ierr)

!Arguments-------------------------
 real(dp) ABI_ASYNC, intent(inout) :: xval(:)
 integer, intent(in) :: root, comm
 integer, intent(out) :: request, ierr

! *************************************************************************

 ierr=0
#ifdef HAVE_MPI_IBCAST
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_IBCAST(xval, product(shape(xval)), MPI_DOUBLE_PRECISION, root, comm, request, ierr)
   xmpi_count_requests = xmpi_count_requests + 1
   return
 end if
#endif

 ! Call the blocking version and return null request.
 call xmpi_bcast(xval, root, comm, ierr)
 request = xmpi_request_null

end subroutine xmpi_ibcast_dp1d
!!***

!!****f* ABINIT/xmpi_ibcast_dp2d
!! NAME
!!  xmpi_ibcast_dp2d
!!
!! FUNCTION
!!  Sends data from one processor to others inside comm without blocking
!!  Target: double two-dimensional dp arrays.
!!
!! INPUTS
!!  root: rank of broadcast root (integer)
!!  comm: MPI communicator
!!
!! OUTPUT
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_ibcast_dp2d(xval, root, comm, request, ierr)

!Arguments-------------------------
 real(dp) ABI_ASYNC, intent(inout) :: xval(:, :)
 integer, intent(in) :: root, comm
 integer, intent(out) :: request, ierr

! *************************************************************************

 ierr=0
#ifdef HAVE_MPI_IBCAST
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_IBCAST(xval, product(shape(xval)), MPI_DOUBLE_PRECISION, root, comm, request, ierr)
   xmpi_count_requests = xmpi_count_requests + 1
   return
 end if
#endif

 ! Call the blocking version and return null request.
 call xmpi_bcast(xval, root, comm, ierr)
 request = xmpi_request_null

end subroutine xmpi_ibcast_dp2d
!!***

!!****f* ABINIT/xmpi_ibcast_dp3d
!! NAME
!!  xmpi_ibcast_dp3d
!!
!! FUNCTION
!!  Sends data from one processor to others inside comm without blocking
!!  Target: double three-dimensional dp arrays.
!!
!! INPUTS
!!  root: rank of broadcast root (integer)
!!  comm: MPI communicator
!!
!! OUTPUT
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_ibcast_dp3d(xval, root, comm, request, ierr)

!Arguments-------------------------
 real(dp) ABI_ASYNC, intent(inout) :: xval(:, :, :)
 integer, intent(in) :: root, comm
 integer, intent(out) :: request, ierr

! *************************************************************************

 ierr=0
#ifdef HAVE_MPI_IBCAST
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_IBCAST(xval, product(shape(xval)), MPI_DOUBLE_PRECISION, root, comm, request, ierr)
   xmpi_count_requests = xmpi_count_requests + 1
   return
 end if
#endif

 ! Call the blocking version and return null request.
 call xmpi_bcast(xval, root, comm, ierr)
 request = xmpi_request_null

end subroutine xmpi_ibcast_dp3d
!!***

!!****f* ABINIT/xmpi_ibcast_dp4d
!! NAME
!!  xmpi_ibcast_dp4d
!!
!! FUNCTION
!!  Sends data from one processor to others inside comm without blocking
!!  Target: double three-dimensional dp arrays.
!!
!! INPUTS
!!  root: rank of broadcast root (integer)
!!  comm: MPI communicator
!!
!! OUTPUT
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_ibcast_dp4d(xval, root, comm, request, ierr)

!Arguments-------------------------
 real(dp) ABI_ASYNC, intent(inout) :: xval(:, :, :, :)
 integer, intent(in) :: root, comm
 integer, intent(out) :: request, ierr

! *************************************************************************

 ierr=0
#ifdef HAVE_MPI_IBCAST
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_IBCAST(xval, product(shape(xval)), MPI_DOUBLE_PRECISION, root, comm, request, ierr)
   xmpi_count_requests = xmpi_count_requests + 1
   return
 end if
#endif

 ! Call the blocking version and return null request.
 call xmpi_bcast(xval, root, comm, ierr)
 request = xmpi_request_null

end subroutine xmpi_ibcast_dp4d
!!***

!!****f* ABINIT/xmpi_ibcast_dpc2d
!! NAME
!!  xmpi_ibcast_dpc2d
!!
!! FUNCTION
!!  Sends data from one processor to others inside comm without blocking
!!  Target: double precision two-dimensional complex arrays.
!!
!! INPUTS
!!  root: rank of broadcast root (integer)
!!  comm: MPI communicator
!!
!! OUTPUT
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_ibcast_dpc2d(xval, root, comm, request, ierr)

!Arguments-------------------------
 complex(dp) ABI_ASYNC, intent(inout) :: xval(:, :)
 integer, intent(in) :: root, comm
 integer, intent(out) :: request, ierr

! *************************************************************************

 ierr=0
#ifdef HAVE_MPI_IBCAST
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_IBCAST(xval, product(shape(xval)), MPI_DOUBLE_COMPLEX, root, comm, request, ierr)
   xmpi_count_requests = xmpi_count_requests + 1
   return
 end if
#endif

 ! Call the blocking version and return null request.
 call xmpi_bcast(xval, root, comm, ierr)
 request = xmpi_request_null

end subroutine xmpi_ibcast_dpc2d
!!***

!!****f* ABINIT/xmpi_ibcast_spc2d
!! NAME
!!  xmpi_ibcast_spc2d
!!
!! FUNCTION
!!  Sends data from one processor to others inside comm without blocking
!!  Target: single precision two-dimensional complex arrays.
!!
!! INPUTS
!!  root: rank of broadcast root (integer)
!!  comm: MPI communicator
!!
!! OUTPUT
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_ibcast_spc2d(xval, root, comm, request, ierr)

!Arguments-------------------------
 complex(sp) ABI_ASYNC, intent(inout) :: xval(:, :)
 integer, intent(in) :: root, comm
 integer, intent(out) :: request, ierr

! *************************************************************************

 ierr=0
#ifdef HAVE_MPI_IBCAST
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_IBCAST(xval, product(shape(xval)), MPI_COMPLEX, root, comm, request, ierr)
   xmpi_count_requests = xmpi_count_requests + 1
   return
 end if
#endif

 ! Call the blocking version and return null request.
 call xmpi_bcast(xval, root, comm, ierr)
 request = xmpi_request_null

end subroutine xmpi_ibcast_spc2d
!!***
