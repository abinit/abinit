!{\src2tex{textfont=tt}}
!!****f* ABINIT/xmpi_allgatherv_int2d
!! NAME
!!  xmpi_allgatherv_int2d
!!
!! FUNCTION
!!  This module contains functions that calls MPI routine,
!!  if we compile the code using the MPI CPP flags.
!!  xmpi_allgatherv is the generic function.
!!
!! COPYRIGHT
!!  Copyright (C) 2001-2024 ABINIT group (AR,XG)
!!  This file is distributed under the terms of the
!!  GNU General Public License, see ~ABINIT/COPYING
!!  or http://www.gnu.org/copyleft/gpl.txt .
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_int2d(xval,nelem,recvbuf,recvcounts,displs,comm,ier)

!Arguments-------------------------
 integer, DEV_CONTARRD intent(in) :: xval(:,:)
 integer, DEV_CONTARRD intent(inout) :: recvbuf(:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,comm
 integer,intent(out) :: ier

!Local variables--------------
 integer :: cc,dd,sz1

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_ALLGATHERV(xval,nelem,MPI_INTEGER,recvbuf,recvcounts,displs,&
&   MPI_INTEGER,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz1=size(xval,1)
   dd=0;if (size(displs)>0) dd=displs(1)/sz1
   cc=size(xval,2);if (size(recvcounts)>0) cc=recvcounts(1)/sz1
   recvbuf(:,dd+1:dd+cc)=xval(:,1:cc)
#if defined HAVE_MPI
 end if
#endif
end subroutine xmpi_allgatherv_int2d
!!***

!!****f* ABINIT/xmpi_allgatherv_int
!! NAME
!!  xmpi_allgatherv_int
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: one-dimensional integer arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_int(xval,nelem,recvbuf,recvcounts,displs,comm,ier)

!Arguments-------------------------
 integer, DEV_CONTARRD intent(in) :: xval(:)
 integer, DEV_CONTARRD intent(inout) :: recvbuf(:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,comm
 integer,intent(out) :: ier

!Local variables-------------------
 integer :: cc,dd

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_ALLGATHERV(xval,nelem,MPI_INTEGER,recvbuf,recvcounts,displs,&
&   MPI_INTEGER,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   dd=0;if (size(displs)>0) dd=displs(1)
   cc=size(xval);if (size(recvcounts)>0) cc=recvcounts(1)
   recvbuf(dd+1:dd+cc)=xval(1:cc)
#if defined HAVE_MPI
 end if
#endif
end subroutine xmpi_allgatherv_int
!!***

!!****f* ABINIT/xmpi_allgatherv_int1_dp1
!! NAME
!!  xmpi_allgatherv_int1_dp1
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target :  one-dimensional integer arrray and one-dimensionnal dp array
!!
!! INPUTS
!!  buf_int=buffer integer array that is going to be gathered
!!  buf_int_size=size of buf_int array
!!  buf_dp=buffer dp array that is going to be gathered
!!  buf_dp_size=size of buf_dp array
!!  comm=MPI communicator
!!
!! OUTPUT
!!  buf_int_all=buffer integer array gathered
!!  buf_int_size_all=size of buffer integer array gathered
!!  buf_dp_all=buffer dp array gathered
!!  buf_dp_size_all=size of buffer dp array gathered
!!  ier=exit status, a non-zero value meaning there is an error
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_int1_dp1(buf_int,buf_int_size,buf_dp,buf_dp_size,&
&       buf_int_all,buf_int_size_all,buf_dp_all,buf_dp_size_all,comm,ier)

!Arguments-------------------------
!scalars
 integer,intent(in) :: buf_dp_size,buf_int_size,comm
 integer,intent(out) :: buf_dp_size_all,buf_int_size_all,ier
!arrays
 integer, intent(in) :: buf_int(:)
 integer,allocatable,target,intent(out) :: buf_int_all(:)
 real(dp),intent(in) :: buf_dp(:)
 real(dp),allocatable,target,intent(out) :: buf_dp_all(:)

!Local variables--------------
!scalars
 integer :: buf_pack_size,ierr,ii,iproc,istart_dp,istart_int,lg,lg1,lg2,lg_dp,lg_int
 integer :: nproc,position,totalbufcount
 logical,parameter :: use_pack=.false.
!arrays
 integer :: buf_size(2),pos(3)
 integer ,allocatable :: buf_int_size1(:),buf_dp_size1(:)
 integer,allocatable :: count_dp(:),count_int(:),count_size(:),counts(:)
 integer,allocatable :: disp_dp(:),disp_int(:),displ(:),displ_dp(:),displ_int(:)
 integer,allocatable :: pos_all(:)
 integer,pointer :: outbuf_int(:)
 real(dp),pointer:: outbuf_dp(:)
 character,allocatable :: buf_pack(:),buf_pack_tot(:)

! *************************************************************************

 ier=0

#if defined HAVE_MPI
 if (comm/=MPI_COMM_SELF.and.comm/=MPI_COMM_NULL) then

   nproc=xmpi_comm_size(comm)

!First version: using 2 allgather (one for ints, another for reals)
!------------------------------------------------------------------
   if (.not.use_pack) then

!  Prepare communications
     ABI_MALLOC(count_int,(nproc))
     ABI_MALLOC(disp_int,(nproc))
     ABI_MALLOC(count_dp,(nproc))
     ABI_MALLOC(disp_dp,(nproc))
     ABI_MALLOC(count_size,(2*nproc))
     buf_size(1)=buf_int_size; buf_size(2)=buf_dp_size
     call xmpi_allgather(buf_size,2,count_size,comm,ier)
     do iproc=1,nproc
       count_int(iproc)=count_size(2*iproc-1)
       count_dp(iproc)=count_size(2*iproc)
     end do
     disp_int(1)=0;disp_dp(1)=0
     do ii=2,nproc
       disp_int(ii)=disp_int(ii-1)+count_int(ii-1)
       disp_dp (ii)=disp_dp (ii-1)+count_dp (ii-1)
     end do
     buf_int_size_all=sum(count_int)
     buf_dp_size_all =sum(count_dp)

     ABI_STAT_MALLOC(buf_int_all,(buf_int_size_all), ier)
     if (ier/=0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_allgatherv')
     ABI_STAT_MALLOC(buf_dp_all ,(buf_dp_size_all), ierr)
     if (ier/=0) call xmpi_abort(msg='error allocating buf_dp_all in xmpi_allgatherv')

!  Communicate (one call for integers, one call for reals)
     call xmpi_allgatherv(buf_int,buf_int_size,buf_int_all,count_int,disp_int,comm,ierr)
     call xmpi_allgatherv(buf_dp,buf_dp_size,buf_dp_all,count_dp,disp_dp,comm,ierr)

!  Release the memory
     ABI_FREE(count_int)
     ABI_FREE(disp_int)
     ABI_FREE(count_dp)
     ABI_FREE(disp_dp)
     ABI_FREE(count_size)

!2nd version: using 1 allgather (with MPI_PACK)
!-----------------------------------------------------------------
   else

!  Compute size of message
     call MPI_PACK_SIZE(buf_int_size,MPI_INTEGER,comm,lg1,ier)
     call MPI_PACK_SIZE(buf_dp_size,MPI_DOUBLE_PRECISION,comm,lg2,ier)
     lg=lg1+lg2

!  Pack data to be sent
     position=0 ; buf_pack_size=lg1+lg2
     ABI_MALLOC(buf_pack,(buf_pack_size))
     call MPI_PACK(buf_int,buf_int_size,MPI_INTEGER,buf_pack,buf_pack_size,position,comm,ier)
     call MPI_PACK(buf_dp,buf_dp_size,MPI_DOUBLE_PRECISION,buf_pack,buf_pack_size,position,comm,ier)

!  Gather size of all packed messages
     ABI_MALLOC(pos_all,(nproc*3))
     ABI_MALLOC(counts,(nproc))
     ABI_MALLOC(buf_int_size1,(nproc))
     ABI_MALLOC(buf_dp_size1,(nproc))
     ABI_MALLOC(displ,(nproc))
     ABI_MALLOC(displ_int,(nproc))
     ABI_MALLOC(displ_dp,(nproc))
     pos(1)=position;pos(2)=buf_int_size;pos(3)=buf_dp_size
     call MPI_ALLGATHER(pos,3,MPI_INTEGER,pos_all,3,MPI_INTEGER,comm,ier)
     ii=1
     do iproc=1,nproc
       counts(iproc)=pos_all(ii);ii=ii+1
       buf_int_size1(iproc)=pos_all(ii);ii=ii+1
       buf_dp_size1(iproc)=pos_all(ii);ii=ii+1
     end do

     displ(1)=0 ; displ_int(1)=0 ; displ_dp(1)=0
     do iproc=2,nproc
       displ(iproc)=displ(iproc-1)+counts(iproc-1)
       displ_int(iproc)=displ_int(iproc-1)+buf_int_size1(iproc-1)
       displ_dp(iproc)=displ_dp(iproc-1)+buf_dp_size1(iproc-1)
     end do

     totalbufcount=displ(nproc)+counts(nproc)
     ABI_STAT_MALLOC(buf_pack_tot,(totalbufcount), ier)
     if (ier/= 0) call xmpi_abort(msg='error allocating totalbufcount in xmpi_allgatherv')
     buf_int_size_all=sum(buf_int_size1)
     buf_dp_size_all=sum(buf_dp_size1)
     ABI_STAT_MALLOC(buf_int_all,(buf_int_size_all), ier)
     if (ier/=0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_allgatherv')
     ABI_STAT_MALLOC(buf_dp_all,(buf_dp_size_all), ier)
     if (ier/=0) call xmpi_abort(msg='error allocating buf_dp_size_all in xmpi_allgatherv')

!  Gather all packed messages
     call MPI_ALLGATHERV(buf_pack,position,MPI_PACKED,buf_pack_tot,counts,displ,MPI_PACKED,comm,ier)
     position=0
     do iproc=1,nproc
       lg_int=buf_int_size1(iproc); lg_dp=buf_dp_size1(iproc)
       istart_int=displ_int(iproc); istart_dp=displ_dp(iproc)
       outbuf_int=>buf_int_all(istart_int+1:istart_int+lg_int)
       call MPI_UNPACK(buf_pack_tot,totalbufcount,position, outbuf_int,&
&       lg_int,MPI_INTEGER,comm,ier)
       outbuf_dp=>buf_dp_all(istart_dp+1:istart_dp+lg_dp)
       call MPI_UNPACK(buf_pack_tot,totalbufcount,position,outbuf_dp,&
&       lg_dp,MPI_DOUBLE_PRECISION,comm,ier)
     end do

!  Release the memory
     ABI_FREE(pos_all)
     ABI_FREE(counts)
     ABI_FREE(buf_int_size1)
     ABI_FREE(buf_dp_size1)
     ABI_FREE(displ)
     ABI_FREE(displ_int)
     ABI_FREE(displ_dp)
     ABI_FREE(buf_pack_tot)
     ABI_FREE(buf_pack)

   end if
 else if (comm==MPI_COMM_SELF) then
#endif

!Sequential version
   ABI_STAT_MALLOC(buf_int_all,(buf_int_size), ier)
   if (ier/=0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_allgatherv')
   ABI_STAT_MALLOC(buf_dp_all,(buf_dp_size), ier)
   if (ier/=0) call xmpi_abort(msg='error allocating buf_dp_all in xmpi_allgatherv')

   buf_int_all(:)=buf_int(:)
   buf_dp_all(:)=buf_dp(:)
   buf_int_size_all=buf_int_size
   buf_dp_size_all=buf_dp_size

#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_allgatherv_int1_dp1
!!***

!!****f* ABINIT/xmpi_allgatherv_dp
!! NAME
!!  xmpi_allgatherv_dp
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: one-dimensional double precision arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_dp(xval,nelem,recvbuf,recvcounts,displs,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:)
 real(dp), DEV_CONTARRD intent(inout) :: recvbuf(:)
 integer,intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,comm
 integer,intent(out) :: ier

!Local variables--------------
 integer :: cc,dd

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_ALLGATHERV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   dd=0;if (size(displs)>0) dd=displs(1)
   cc=size(xval);if (size(recvcounts)>0) cc=recvcounts(1)
   recvbuf(dd+1:dd+cc)=xval(1:cc)
#if defined HAVE_MPI
 end if
#endif
end subroutine xmpi_allgatherv_dp
!!***

!!****f* ABINIT/xmpi_allgatherv_dp2d
!! NAME
!!  xmpi_allgatherv_dp2d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision two-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_dp2d(xval,nelem,recvbuf,recvcounts,displs,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:)
 real(dp), DEV_CONTARRD intent(inout) :: recvbuf(:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,comm
 integer,intent(out) :: ier

!Local variables--------------
 integer :: cc,dd,sz1

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_ALLGATHERV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz1=size(xval,1)
   dd=0;if (size(displs)>0) dd=displs(1)/sz1
   cc=size(xval,2);if (size(recvcounts)>0) cc=recvcounts(1)/sz1
   recvbuf(:,dd+1:dd+cc)=xval(:,1:cc)
#if defined HAVE_MPI
 end if
#endif
end subroutine xmpi_allgatherv_dp2d
!!***

!!****f* ABINIT/xmpi_allgatherv_dp3d
!! NAME
!!  xmpi_allgatherv_dp3d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision three-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_dp3d(xval,nelem,recvbuf,recvcounts,displs,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:,:)
 real(dp), DEV_CONTARRD intent(inout) :: recvbuf(:,:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,comm
 integer,intent(out) :: ier

!Local variables--------------
 integer :: cc,dd,sz12

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_ALLGATHERV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz12=size(xval,1)*size(xval,2)
   dd=0;if (size(displs)>0) dd=displs(1)/sz12
   cc=size(xval,3);if (size(recvcounts)>0) cc=recvcounts(1)/sz12
   recvbuf(:,:,dd+1:dd+cc)=xval(:,:,1:cc)
#if defined HAVE_MPI
 end if
#endif
end subroutine xmpi_allgatherv_dp3d
!!***

!!****f* ABINIT/xmpi_allgatherv_dp4d
!! NAME
!!  xmpi_allgatherv_dp4d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision four-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_dp4d(xval,nelem,recvbuf,recvcounts,displs,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:,:,:)
 real(dp), DEV_CONTARRD intent(inout)   :: recvbuf(:,:,:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,comm
 integer,intent(out) :: ier

!Local variables-------------------
 integer :: cc,dd,sz123

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_ALLGATHERV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz123=size(xval,1)*size(xval,2)*size(xval,3)
   dd=0;if (size(displs)>0) dd=displs(1)/sz123
   cc=size(xval,4);if (size(recvcounts)>0) cc=recvcounts(1)/sz123
   recvbuf(:,:,:,dd+1:dd+cc)=xval(:,:,:,1:cc)
#if defined HAVE_MPI
 end if
#endif
end subroutine xmpi_allgatherv_dp4d
!!***

!!****f* ABINIT/xmpi_allgatherv_dp5d
!! NAME
!!  xmpi_allgatherv_dp5d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision six-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_dp5d(xval,nelem,recvbuf,recvcounts,displs,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:,:,:,:)
 real(dp), DEV_CONTARRD intent(inout)   :: recvbuf(:,:,:,:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,comm
 integer,intent(out) :: ier

!Local variables-------------------
 integer :: cc,dd,sz1234

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_ALLGATHERV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz1234=size(xval,1)*size(xval,2)*size(xval,3)*size(xval,4)
   dd=0;if (size(displs)>0) dd=displs(1)/sz1234
   cc=size(xval,5);if (size(recvcounts)>0) cc=recvcounts(1)/sz1234
   recvbuf(:,:,:,:,dd+1:dd+cc)=xval(:,:,:,:,1:cc)
#if defined HAVE_MPI
 end if
#endif
end subroutine xmpi_allgatherv_dp5d
!!***


!!****f* ABINIT/xmpi_allgatherv_dp6d
!! NAME
!!  xmpi_allgatherv_dp6d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision six-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_dp6d(xval,nelem,recvbuf,recvcounts,displs,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:,:,:,:,:)
 real(dp), DEV_CONTARRD intent(inout)   :: recvbuf(:,:,:,:,:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,comm
 integer,intent(out) :: ier

!Local variables-------------------
 integer :: cc,dd,sz12345

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_ALLGATHERV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz12345=size(xval,1)*size(xval,2)*size(xval,3)*size(xval,4)*size(xval,5)
   dd=0;if (size(displs)>0) dd=displs(1)/sz12345
   cc=size(xval,6);if (size(recvcounts)>0) cc=recvcounts(1)/sz12345
   recvbuf(:,:,:,:,:,dd+1:dd+cc)=xval(:,:,:,:,:,1:cc)
#if defined HAVE_MPI
 end if
#endif
end subroutine xmpi_allgatherv_dp6d
!!***

!!****f* ABINIT/xmpi_allgatherv_coeff2d
!! NAME
!!  xmpi_allgatherv_coeff2d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: coeff2_type 1D-structure
!!
!! INPUTS
!!  xval_in = coeff2d_type array structure
!!  comm= MPI communicator
!!
!! OUTPUT
!!  xval_out = coeff2d_type array structure
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_coeff2d(xval_in,xval_out,comm,ierr)

!Arguments ------------------------------------
!scalars
 integer,intent(in) :: comm
 integer,intent(out)   :: ierr
!arrays
 type(coeff2_type),intent(in) :: xval_in(:)
 type(coeff2_type),intent(out) :: xval_out(:)

!Local variables-------------------------------
!scalars
 integer :: ii,n1,n2
#if defined HAVE_MPI
 integer :: buf_int_size,buf_int_size_all,buf_dp_size,buf_dp_size_all
 integer :: i2,indx_int,indx_dp,nb,nb_out,nproc
#endif
!arrays
#if defined HAVE_MPI
 integer, allocatable ::  buf_int(:),buf_int_all(:)
 integer, allocatable :: dimxval(:,:)
 real(dp),allocatable :: buf_dp(:),buf_dp_all(:)
#endif

! *************************************************************************

 ierr=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then

   nproc=xmpi_comm_size(comm)
   nb = size(xval_in,1)

   if (comm==MPI_COMM_SELF.or.nproc==1) then
     do ii=1,nb
       n1=size(xval_in(ii)%value,1)
       n2=size(xval_in(ii)%value,2)
       if (allocated(xval_out(ii)%value)) then
         ABI_FREE(xval_out(ii)%value)
       end if
       ABI_STAT_MALLOC(xval_out(ii)%value,(n1,n2), ierr)
       if (ierr/= 0) call xmpi_abort(msg='error allocating xval_out%value in xmpi_allgatherv')
       xval_out(ii)%value=xval_in(ii)%value
     end do
     return
   end if

   buf_dp_size=0
   ABI_MALLOC(dimxval,(nb,2))
   do ii=1,nb
     dimxval(ii,1)=size(xval_in(ii)%value,dim=1)
     dimxval(ii,2)=size(xval_in(ii)%value,dim=2)
     buf_dp_size=buf_dp_size+dimxval(ii,1)*dimxval(ii,2)
   end do

   buf_int_size=2*nb;
   ABI_STAT_MALLOC(buf_int,(buf_int_size), ierr)
   if (ierr/= 0) call xmpi_abort(msg='error allocating buf_int in xmpi_allgatherv')
   indx_int=1
   do ii=1,nb
     buf_int(indx_int  )=dimxval(ii,1)
     buf_int(indx_int+1)=dimxval(ii,2)
     indx_int=indx_int+2
   end do

   ABI_STAT_MALLOC(buf_dp,(buf_dp_size) ,ierr)
   if (ierr/= 0) call xmpi_abort(msg='error allocating buf_dp_size in xmpi_allgatherv')
   indx_dp=1
   do ii=1,nb
     n1=dimxval(ii,1); n2=dimxval(ii,2)
     do i2=1,n2
       buf_dp(indx_dp:indx_dp+n1-1)=xval_in(ii)%value(1:n1,i2)
       indx_dp=indx_dp+n1
     end do
   end do

   call xmpi_allgatherv(buf_int,buf_int_size,buf_dp,buf_dp_size,buf_int_all, &
&   buf_int_size_all,buf_dp_all,buf_dp_size_all,comm,ierr)


   nb_out=buf_int_size_all/2

   indx_int=1;indx_dp=1
   do ii=1,nb_out
     n1=buf_int_all(indx_int)
     n2=buf_int_all(indx_int+1)
     indx_int=indx_int+2
     if (allocated(xval_out(ii)%value)) then
       ABI_FREE(xval_out(ii)%value)
     end if
     ABI_STAT_MALLOC(xval_out(ii)%value,(n1,n2), ierr)
     if (ierr/= 0) call xmpi_abort(msg='error allocating xval_out%value in xmpi_allgatherv')
     do i2=1,n2
       xval_out(ii)%value(1:n1,i2)=buf_dp_all(indx_dp:indx_dp+n1-1)
       indx_dp=indx_dp+n1
     end do
   end do


   ABI_FREE(buf_dp_all)
   ABI_FREE(buf_int_all)
   ABI_FREE(buf_int)
   ABI_FREE(buf_dp)
   ABI_FREE(dimxval)

 end if

#else
 do ii=1,size(xval_in,1)
   n1=size(xval_in(ii)%value,1)
   n2=size(xval_in(ii)%value,2)
   if (allocated(xval_out(ii)%value)) then
     ABI_FREE(xval_out(ii)%value)
   end if
   ABI_STAT_MALLOC(xval_out(ii)%value,(n1,n2), ierr)
   if (ierr/= 0) call xmpi_abort(msg='error allocating xval_out%value in xmpi_allgatherv')
   xval_out(ii)%value=xval_in(ii)%value
 end do
#endif

end subroutine xmpi_allgatherv_coeff2d
!!***

!!****f* ABINIT/xmpi_allgatherv_coeff2d_indx
!! NAME
!!  xmpi_allgatherv_coeff2d_indx
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: coeff2_type 1D-structure
!!          use of an indirect index to sort data
!!
!! INPUTS
!!  xval_in = coeff2d_type array structure
!!  comm= MPI communicator
!!  indx= gives the indexes of xval_in in xval_out.
!!        xval_in(i) will be transfered in xval_out(indx(i))
!!
!! OUTPUT
!!  xval_out = coeff2d_type array structure
!!  ierr= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!
!! PARENTS
!!
!! CHILDREN
!!      xmpi_allgatherv
!!
!! SOURCE

subroutine xmpi_allgatherv_coeff2d_indx(xval_in,xval_out,comm,indx,ierr)

!Arguments ------------------------------------
!scalars
 integer,intent(in) :: comm
 integer,intent(out)   :: ierr
!arrays
 integer,intent(in) :: indx(:)
 type(coeff2_type),intent(in) :: xval_in(:)
 type(coeff2_type),intent(out) :: xval_out(:)

!Local variables-------------------------------
!scalars
 integer :: ii,ival,n1,n2,nb
#if defined HAVE_MPI
 integer :: buf_int_size,buf_int_size_all,buf_dp_size,buf_dp_size_all
 integer :: i2,indx_int,indx_dp,nb_out,nproc
#endif
!arrays
#if defined HAVE_MPI
 integer, allocatable :: buf_int(:),buf_int_all(:)
 integer, allocatable :: dimxval(:,:)
 real(dp),allocatable :: buf_dp(:),buf_dp_all(:)
#endif

! *************************************************************************

 ierr=0 ; nb = size(xval_in,1)

#if defined HAVE_MPI
 if (comm == MPI_COMM_NULL) return
 nproc=xmpi_comm_size(comm)
 if (comm==MPI_COMM_SELF.or.nproc==1) then
#endif
   do ii=1,nb
     n1=size(xval_in(ii)%value,1)
     n2=size(xval_in(ii)%value,2)
     ival=indx(ii)
     if (allocated(xval_out(ival)%value)) then
       ABI_FREE(xval_out(ival)%value)
     end if
     ABI_STAT_MALLOC(xval_out(ival)%value,(n1,n2), ierr)
     if (ierr/= 0) call xmpi_abort(msg='error allocating xval_out%value in xmpi_allgatherv')
     xval_out(ii)%value=xval_in(ival)%value
   end do
   return

#if defined HAVE_MPI
 end if

 buf_dp_size=0
 ABI_STAT_MALLOC(dimxval,(nb,2), ierr)
 if (ierr/= 0) call xmpi_abort(msg='error allocating dimxval in xmpi_allgatherv')
 do ii=1,nb
   dimxval(ii,1)=size(xval_in(ii)%value,dim=1)
   dimxval(ii,2)=size(xval_in(ii)%value,dim=2)
   buf_dp_size=buf_dp_size+dimxval(ii,1)*dimxval(ii,2)
 end do

 buf_int_size=3*nb
 ABI_STAT_MALLOC(buf_int,(buf_int_size), ierr)
 if (ierr/= 0) call xmpi_abort(msg='error allocating buf_int in xmpi_allgatherv')
 indx_int=1
 do ii=1,nb
   buf_int(indx_int  )=dimxval(ii,1)
   buf_int(indx_int+1)=dimxval(ii,2)
   buf_int(indx_int+2)=indx(ii)
   indx_int=indx_int+3
 end do

 ABI_STAT_MALLOC(buf_dp,(buf_dp_size), ierr)
 if (ierr/= 0) call xmpi_abort(msg='error allocating buf_dp in xmpi_allgatherv')
 indx_dp=1
 do ii=1,nb
   n1=dimxval(ii,1); n2=dimxval(ii,2)
   do i2=1,n2
     buf_dp(indx_dp:indx_dp+n1-1)=xval_in(ii)%value(1:n1,i2)
     indx_dp=indx_dp+n1
   end do
 end do

 call xmpi_allgatherv(buf_int,buf_int_size,buf_dp,buf_dp_size,buf_int_all, &
&   buf_int_size_all,buf_dp_all,buf_dp_size_all,comm,ierr)

 nb_out=buf_int_size_all/3
 indx_int=1;indx_dp=1
 do ii=1,nb_out
   n1=buf_int_all(indx_int)
   n2=buf_int_all(indx_int+1)
   ival=buf_int_all(indx_int+2)
   indx_int=indx_int+3
   if (allocated(xval_out(ival)%value)) then
     ABI_FREE(xval_out(ival)%value)
   end if
   ABI_STAT_MALLOC(xval_out(ival)%value,(n1,n2), ierr)
   if (ierr/= 0) call xmpi_abort(msg='error allocating xval_out%value in xmpi_allgatherv')
   do i2=1,n2
     xval_out(ival)%value(1:n1,i2)=buf_dp_all(indx_dp:indx_dp+n1-1)
     indx_dp=indx_dp+n1
   end do
 end do

 ABI_FREE(buf_dp_all)
 ABI_FREE(buf_int_all)
 ABI_FREE(buf_int)
 ABI_FREE(buf_dp)
 ABI_FREE(dimxval)
#endif

end subroutine xmpi_allgatherv_coeff2d_indx
!!***
