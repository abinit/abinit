!{\src2tex{textfont=tt}}
!!****f* ABINIT/xmpi_sum_master
!! NAME
!!  xmpi_sum_master
!!
!! FUNCTION
!!  This module contains functions that calls MPI routine,
!!  if we compile the code using the MPI CPP flags.
!!  xmpi_sum_master is the generic function.
!!
!! COPYRIGHT
!!  Copyright (C) 2001-2024 ABINIT group (AR,XG,MB)
!!  This file is distributed under the terms of the
!!  GNU General Public License, see ~ABINIT/COPYING
!!  or http://www.gnu.org/copyleft/gpl.txt .
!!
!! NOTES
!!  The workspace array xsum is filled to zeros to avoid SIFPE in [mpiio][t28_MPI4][np=4] on tikal_gnu_4.9_mpich
!!  On this bot, the code is compiled with -ffpe-trap and the illegal operation in the MPI library
!!  make tests using xmpi_sum_master abort.
!!  Strictly speaking the initialization is not needed because xsum has intent(out) --> bug in mpich3-3.1.3 (gcc492)
!!
!! SOURCE

!!***

!!****f* ABINIT/xmpi_sum_master_int
!! NAME
!!  xmpi_sum_master_int
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: integer scalars.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_int(xval,master,comm,ier)

!Arguments-------------------------
 integer,intent(inout) :: xval
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out)   :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: nproc_space_comm
 integer :: arr_xsum(1)
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
!    Accumulate xval on all proc. in comm
     call MPI_REDUCE([xval],arr_xsum,1,MPI_INTEGER,MPI_SUM,master,comm,ier)
     xval = arr_xsum(1)
   end if
 end if
#endif
end subroutine xmpi_sum_master_int
!!***

!!****f* ABINIT/xmpi_sum_master_dp
!! NAME
!!  xmpi_sum_master_p
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: integer scalars.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_dp(xval,master,comm,ier)

!Arguments-------------------------
 real(dp),intent(inout) :: xval
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out)   :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: nproc_space_comm
 real(dp) :: arr_xsum(1)
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
!    Accumulate xval on all proc. in comm
     call MPI_REDUCE([xval],arr_xsum,1,MPI_DOUBLE_PRECISION,MPI_SUM,master,comm,ier)
     xval = arr_xsum(1)
   end if
 end if
#endif
end subroutine xmpi_sum_master_dp
!!***

!!****f* ABINIT/xmpi_sum_master_int2d
!! NAME
!!  xmpi_sum_master_int2d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: two-dimensional integer arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_int2d(xval,master,comm,ier)

!Arguments ------------------------------------
 integer, DEV_CONTARRD intent(inout) :: xval(:,:)
 integer,intent(in) :: master,comm
 integer,intent(out) :: ier

!Local variables-------------------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
 integer, allocatable :: xsum(:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)

     ABI_STAT_MALLOC(xsum,(n1,n2), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = 0 ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2,MPI_INTEGER,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_INTEGER,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval = xsum
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_int2d
!!***

!!****f* ABINIT/xmpi_sum_master_dp1d
!! NAME
!!  xmpi_sum_master_dp1d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: double precision one-dimensional arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_dp1d(xval,master,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: n1
 real(dp) , allocatable :: xsum(:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
!    Accumulate xval on all proc. in comm
     ABI_STAT_MALLOC(xsum,(n1), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = zero ! See notes
     call MPI_REDUCE(xval,xsum,n1,MPI_DOUBLE_PRECISION,MPI_SUM,master,comm,ier)
     xval (:) = xsum(:)
     ABI_FREE(xsum)
   end if
 end if
#endif
end subroutine xmpi_sum_master_dp1d
!!***

!!****f* ABINIT/xmpi_sum_master_dp2d
!! NAME
!!  xmpi_sum_master_dp2d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: double precision two-dimensional arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_dp2d(xval,master,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
 real(dp) , allocatable :: xsum(:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)

     ABI_STAT_MALLOC(xsum,(n1,n2), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = zero ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2,MPI_DOUBLE_PRECISION,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:) = xsum(:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif
end subroutine xmpi_sum_master_dp2d
!!***

!!****f* ABINIT/xmpi_sum_master_dp3d
!! NAME
!!  xmpi_sum_master_dp3d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: double precision three-dimensional arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_dp3d(xval,master,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3
 integer(kind=int64) :: ntot
 real(dp) , allocatable :: xsum(:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     ABI_STAT_MALLOC(xsum,(n1,n2,n3), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = zero ! See notes


     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3,MPI_DOUBLE_PRECISION,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:) = xsum(:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_dp3d
!!***

!!****f* ABINIT/xmpi_sum_master_dp4d
!! NAME
!!  xmpi_sum_master_dp4d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: double precision four-dimensional arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_dp4d(xval,master,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:,:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4
 integer(kind=int64) :: ntot
 real(dp) , allocatable :: xsum(:,:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)
     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = zero ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4,MPI_DOUBLE_PRECISION,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:,:) = xsum(:,:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_dp4d
!!***

!!****f* ABINIT/xmpi_sum_master_dp5d
!! NAME
!!  xmpi_sum_master_dp5d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: double precision five-dimensional arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_dp5d(xval,master,comm,ier)

!Arguments ------------------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:,:,:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out)   :: ier

!Local variables-------------------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4,n5
 integer(kind=int64) :: ntot
 real(dp), allocatable :: xsum(:,:,:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)
     n5 = size(xval,dim=5)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4,n5), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = zero ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4*n5,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4*n5,MPI_DOUBLE_PRECISION,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:,:,:) = xsum(:,:,:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_dp5d
!!***

!!****f* ABINIT/xmpi_sum_master_dp6d
!! NAME
!!  xmpi_sum_master_dp6d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: double precision six-dimensional arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_dp6d(xval,master,comm,ier)

!Arguments ------------------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:,:,:,:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out) :: ier

!Local variables-------------------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4,n5,n6
 integer(kind=int64) :: ntot
 real(dp), allocatable :: xsum(:,:,:,:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)
     n5 = size(xval,dim=5)
     n6 = size(xval,dim=6)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4,n5,n6), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = zero ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4*n5*n6,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4*n5*n6,MPI_DOUBLE_PRECISION,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:,:,:,:) = xsum(:,:,:,:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_dp6d
!!***

!!****f* ABINIT/xmpi_sum_master_dp7d
!! NAME
!!  xmpi_sum_master_dp7d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: double precision seven-dimensional arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_dp7d(xval,master,comm,ier)

!Arguments ------------------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:,:,:,:,:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out) :: ier

!Local variables-------------------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4,n5,n6,n7
 integer(kind=int64) :: ntot
 real(dp), allocatable :: xsum(:,:,:,:,:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)
     n5 = size(xval,dim=5)
     n6 = size(xval,dim=6)
     n7 = size(xval,dim=7)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4,n5,n6,n7), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = zero ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4*n5*n6*n7,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4*n5*n6*n7,MPI_DOUBLE_PRECISION,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:,:,:,:,:) = xsum(:,:,:,:,:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_dp7d
!!***

!!****f* ABINIT/xmpi_sum_master_int4d
!! NAME
!!  xmpi_sum_master_int4d
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: four-diemnsional integer arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_int4d(xval,master,comm,ier)

!Arguments ------------------------------------
 integer, DEV_CONTARRD intent(inout) :: xval(:,:,:,:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4
 integer(kind=int64) :: ntot
 integer, allocatable :: xsum(:,:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = 0 ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4,MPI_INTEGER,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_INTEGER,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:,:) = xsum(:,:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_int4d
!!***

!!****f* ABINIT/xmpi_sum_master_c1cplx
!! NAME
!!  xmpi_sum_master_c1cplx
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: one-dimensional complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c1cplx(xval,master,comm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: n1,nproc_space_comm
 complex(spc),allocatable :: xsum(:)
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     ABI_STAT_MALLOC(xsum,(n1), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_sp,0_sp) ! See notes
!    Collect xval from processors on master in comm
     call MPI_REDUCE(xval,xsum,n1,MPI_COMPLEX,MPI_SUM,master,comm,ier)
     xval = xsum
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_c1cplx
!!***

!!****f* ABINIT/xmpi_sum_master_c2cplx
!! NAME
!!  xmpi_sum_master_c2cplx
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: two-dimensional complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c2cplx(xval,master,comm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:,:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
 integer :: nproc_space_comm
 complex(spc),allocatable :: xsum(:,:)
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)

     ABI_STAT_MALLOC(xsum,(n1,n2), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_sp,0_sp) ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2,MPI_COMPLEX,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_COMPLEX,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:) = xsum(:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_c2cplx
!!***

!!****f* ABINIT/xmpi_sum_master_c3cplx
!! NAME
!!  xmpi_sum_master_c3cplx
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: three-dimensional complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c3cplx(xval,master,comm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:,:,:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3
 integer(kind=int64) :: ntot
 complex(spc), allocatable :: xsum(:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_sp,0_sp) ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3,MPI_COMPLEX,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_COMPLEX,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:) = xsum(:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_c3cplx
!!***

!!****f* ABINIT/xmpi_sum_master_c4cplx
!! NAME
!!  xmpi_sum_master_c4cplx
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: four-dimensional complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c4cplx(xval,master,comm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:,:,:,:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4
 integer(kind=int64) :: ntot
 integer :: nproc_space_comm
 complex(spc), allocatable :: xsum(:,:,:,:)
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_sp,0_sp) ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4,MPI_COMPLEX,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_COMPLEX,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:,:) = xsum(:,:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif
end subroutine xmpi_sum_master_c4cplx
!!***

!----------------------------------------------------------------------

!!****f* ABINIT/xmpi_sum_master_c5cplx
!! NAME
!!  xmpi_sum_master_c5cplx
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: five-dimensional single precision complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c5cplx(xval,master,comm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:,:,:,:,:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4,n5
 integer(kind=int64) :: ntot
 complex(spc),allocatable :: xsum(:,:,:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)
     n5 = size(xval,dim=5)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4,n5), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_sp,0_sp) ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4*n5,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4*n5,MPI_COMPLEX,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_COMPLEX,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval = xsum
     ABI_FREE(xsum)
   end if
 end if
#endif
end subroutine xmpi_sum_master_c5cplx
!!***

!!****f* ABINIT/xmpi_sum_master_c1dpc
!! NAME
!!  xmpi_sum_master_c1dpc
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: one-dimensional double complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c1dpc(xval,master,comm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout) :: xval(:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: n1
 integer :: nproc_space_comm
 complex(dpc),allocatable :: xsum(:)
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
!    Collect xval from processors on master in comm
     ABI_STAT_MALLOC(xsum,(n1), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_dp,0_dp) ! See notes
     call MPI_REDUCE(xval,xsum,n1,MPI_DOUBLE_COMPLEX,MPI_SUM,master,comm,ier)
     xval (:) = xsum(:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_c1dpc
!!***

!!****f* ABINIT/xmpi_sum_master_c2dpc
!! NAME
!!  xmpi_sum_master_c2dpc
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: two-dimensional double complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c2dpc(xval,master,comm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout) :: xval(:,:)
 integer ,intent(in) :: master
 integer ,intent(in) :: comm
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
 complex(dpc) , allocatable :: xsum(:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)

     ABI_STAT_MALLOC(xsum,(n1,n2), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_dp,0_dp) ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2,MPI_DOUBLE_COMPLEX,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_COMPLEX,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:) = xsum(:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif

end subroutine xmpi_sum_master_c2dpc
!!***

!!****f* ABINIT/xmpi_sum_master_c3dpc
!! NAME
!!  xmpi_sum_master_c3dpc
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: three-dimensional double complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c3dpc(xval,master,comm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout) :: xval(:,:,:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3
 integer(kind=int64) :: ntot
 complex(dpc) , allocatable :: xsum(:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_dp,0_dp) ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3,MPI_DOUBLE_COMPLEX,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_COMPLEX,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:) = xsum(:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif
end subroutine xmpi_sum_master_c3dpc
!!***

!!****f* ABINIT/xmpi_sum_master_c4dpc
!! NAME
!!  xmpi_sum_master_c4dpc
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: four-dimensional double complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c4dpc(xval,master,comm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout) :: xval(:,:,:,:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4
 integer(kind=int64) :: ntot
 complex(dpc) , allocatable :: xsum(:,:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_dp,0_dp) ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4,MPI_DOUBLE_COMPLEX,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_COMPLEX,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:,:) = xsum(:,:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif
end subroutine xmpi_sum_master_c4dpc
!!***

!!****f* ABINIT/xmpi_sum_master_c5dpc
!! NAME
!!  xmpi_sum_master_c5dpc
!!
!! FUNCTION
!!  Reduces values on all processes to a single value.
!!  Target: five-dimensional double complex arrays.
!!
!! INPUTS
!!  master= master MPI node
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_sum_master_c5dpc(xval,master,comm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout) :: xval(:,:,:,:,:)
 integer,intent(in) :: master
 integer,intent(in) :: comm
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4,n5
 integer(kind=int64) :: ntot
 complex(dpc),allocatable :: xsum(:,:,:,:,:)
 integer :: nproc_space_comm
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_NULL) then
   call MPI_COMM_SIZE(comm,nproc_space_comm,ier)
   if (nproc_space_comm /= 1) then
     n1 = size(xval,dim=1)
     n2 = size(xval,dim=2)
     n3 = size(xval,dim=3)
     n4 = size(xval,dim=4)
     n5 = size(xval,dim=5)

     ABI_STAT_MALLOC(xsum,(n1,n2,n3,n4,n5), ier)
     if (ier /= 0) call xmpi_abort(msg='error allocating xsum')
     xsum = (0_dp,0_dp) ! See notes

     !This product of dimensions can be greater than a 32bit integer
     !We use a INT64 to store it. If it is too large, we switch to an
     !alternate routine because MPI<4 doesnt handle 64 bit counts.
     ntot=int(n1*n2*n3*n4*n5,kind=int64)

!    Accumulate xval on all proc. in comm
     if (ntot<=xmpi_maxint32_64) then
       call MPI_reduce(xval,xsum,n1*n2*n3*n4*n5,MPI_DOUBLE_COMPLEX,MPI_SUM,master,comm,ier)
     else
       call xmpi_largetype_create(ntot,MPI_DOUBLE_COMPLEX,my_dt,my_op,MPI_SUM)
       call MPI_reduce(xval,xsum,1,my_dt,my_op,master,comm,ier)
       call xmpi_largetype_free(my_dt,my_op)
     end if

     xval (:,:,:,:,:) = xsum(:,:,:,:,:)
     ABI_FREE(xsum)
   end if
 end if
#endif
end subroutine xmpi_sum_master_c5dpc
!!***
