!{\src2tex{textfont=tt}}
!!****f* ABINIT/xmpi_gatherv
!! NAME
!!  xmpi_gatherv
!!
!! FUNCTION
!!  This module contains functions that calls MPI routine,
!!  if we compile the code using the MPI CPP flags.
!!  xmpi_gatherv is the generic function.
!!
!! COPYRIGHT
!!  Copyright (C) 2001-2024 ABINIT group (MT,GG)
!!  This file is distributed under the terms of the
!!  GNU General Public License, see ~ABINIT/COPYING
!!  or http://www.gnu.org/copyleft/gpl.txt .
!!
!! SOURCE

!!***

!!****f* ABINIT/xmpi_gatherv_int
!! NAME
!!  xmpi_gatherv_int
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: one-dimensional integer arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  root= rank of receiving process
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! SOURCE

subroutine xmpi_gatherv_int(xval,nelem,recvbuf,recvcounts,displs,root,comm,ier)

!Arguments-------------------------
 integer, DEV_CONTARRD intent(in) :: xval(:)
 integer, DEV_CONTARRD intent(inout) :: recvbuf(:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,root,comm
 integer,intent(out) :: ier

!Local variables-------------------
 integer :: cc,dd

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_gatherV(xval,nelem,MPI_INTEGER,recvbuf,recvcounts,displs,&
&   MPI_INTEGER,root,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   dd=0;if (size(displs)>0) dd=displs(1)
   cc=size(xval);if (size(recvcounts)>0) cc=recvcounts(1)
   recvbuf(dd+1:dd+cc)=xval(1:cc)
#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_gatherv_int
!!***

!!****f* ABINIT/xmpi_gatherv_int1_dp1
!! NAME
!!  xmpi_gatherv_int1_dp1
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target :  one-dimensional integer arrray and one-dimensionnal dp array
!!
!! INPUTS
!!  buf_int=buffer integer array that is going to be gathered
!!  buf_int_size=size of buf_int array
!!  buf_dp=buffer dp array that is going to be gathered
!!  buf_dp_size=size of buf_dp array
!!  comm=MPI communicator to be gathered on it
!!  root=rank of receiving process
!!  comm=MPI communicator
!!
!! OUTPUT
!!  buf_int_all=buffer integer array gathered
!!  buf_int_size_all=size of buffer integer array gathered
!!  buf_dp_all=buffer dp array gathered
!!  buf_dp_size_all=size of buffer dp array gathered
!!  ier=exit status, a non-zero value meaning there is an error
!!
!! SOURCE

subroutine xmpi_gatherv_int1_dp1(buf_int,buf_int_size,buf_dp,buf_dp_size, &
&          buf_int_all,buf_int_size_all,buf_dp_all,buf_dp_size_all,root,&
&          comm,ier)

!Arguments-------------------------
!scalars
 integer,intent(in) :: buf_int_size,buf_dp_size,root,comm
 integer,intent(out) :: buf_int_size_all,buf_dp_size_all,ier
!arrays
 integer,intent(in) :: buf_int(:)
 integer,allocatable,target,intent(out) :: buf_int_all(:)
 real(dp),intent(in) :: buf_dp(:)
 real(dp),allocatable,target, intent(out) :: buf_dp_all(:)

!Local variables--------------
!scalars
 integer :: buf_pack_size,ierr,ii,iproc,istart_dp,istart_int
 integer :: lg,lg1,lg2,lg_int,lg_dp,me,nproc,position
 integer :: totalbufcount
 logical,parameter :: use_pack=.false.
!arrays
 integer :: buf_size(2),pos(3)
 integer,allocatable :: buf_dp_size1(:),buf_int_size1(:)
 integer,allocatable :: count_dp(:),count_int(:),count_size(:),counts(:)
 integer,allocatable :: disp_dp(:),disp_int(:),displ(:),displ_dp(:),displ_int(:)
 integer,allocatable :: pos_all(:)
 integer,pointer:: outbuf_int(:)
 real(dp),pointer :: outbuf_dp(:)
 character,allocatable :: buf_pack(:),buf_pack_tot(:)

! *************************************************************************

 ier=0

#if defined HAVE_MPI
 if (comm/=MPI_COMM_SELF.and.comm/=MPI_COMM_NULL) then

   nproc=xmpi_comm_size(comm)

!First version: using 2 allgather (one for ints, another for reals)
!------------------------------------------------------------------
   if (.not.use_pack) then

!  Prepare communications
     ABI_MALLOC(count_int,(nproc))
     ABI_MALLOC(disp_int,(nproc))
     ABI_MALLOC(count_dp,(nproc))
     ABI_MALLOC(disp_dp,(nproc))
     ABI_MALLOC(count_size,(2*nproc))
     buf_size(1)=buf_int_size;buf_size(2)=buf_dp_size
     call xmpi_allgather(buf_size,2, count_size,comm,ier)
     do iproc=1,nproc
       count_int(iproc)=count_size(2*iproc-1)
       count_dp(iproc)=count_size(2*iproc)
     end do
     disp_int(1)=0;disp_dp(1)=0
     do ii=2,nproc
       disp_int(ii)=disp_int(ii-1)+count_int(ii-1)
       disp_dp (ii)=disp_dp (ii-1)+count_dp (ii-1)
     end do
     buf_int_size_all=sum(count_int)
     buf_dp_size_all =sum(count_dp)
     ABI_STAT_MALLOC(buf_int_all,(buf_int_size_all), ier)
     if (ier/= 0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_gatherv')
     ABI_STAT_MALLOC(buf_dp_all ,(buf_dp_size_all), ier)
     if (ier/= 0) call xmpi_abort(msg='error allocating buf_dp_all in xmpi_gatherv')

!  Communicate (one call for integers, one call for reals)
     call xmpi_gatherv(buf_int,buf_int_size,buf_int_all,count_int,disp_int,root,comm,ierr)
     call xmpi_gatherv(buf_dp,buf_dp_size,buf_dp_all,count_dp,disp_dp,root,comm,ierr)

!  Release the memory
     ABI_FREE(count_int)
     ABI_FREE(disp_int)
     ABI_FREE(count_dp)
     ABI_FREE(disp_dp)
     ABI_FREE(count_size)

!2nd version: using 1 allgather (with MPI_PACK)
!-----------------------------------------------------------------
   else

     me=xmpi_comm_rank(comm)

!  Compute size of message
     call MPI_PACK_SIZE(buf_int_size,MPI_INTEGER,comm,lg1,ier)
     call MPI_PACK_SIZE(buf_dp_size,MPI_DOUBLE_PRECISION,comm,lg2,ier)
     lg=lg1+lg2

!  Pack data to be sent
     position=0;buf_pack_size=lg1+lg2
     ABI_STAT_MALLOC(buf_pack,(buf_pack_size), ier)
     if (ier/= 0) call xmpi_abort(msg='error allocating buf_pack xmpi_gatherv')
     call MPI_PACK(buf_int,buf_int_size,MPI_INTEGER,buf_pack,buf_pack_size,position,comm,ier)
     call MPI_PACK(buf_dp,buf_dp_size,MPI_DOUBLE_PRECISION,buf_pack,buf_pack_size,position,comm,ier)

!  Gather size of all packed messages
     ABI_MALLOC(pos_all,(nproc*3))
     ABI_MALLOC(counts,(nproc))
     ABI_MALLOC(buf_int_size1,(nproc))
     ABI_MALLOC(buf_dp_size1,(nproc))
     ABI_MALLOC(displ,(nproc))
     ABI_MALLOC(displ_int,(nproc))
     ABI_MALLOC(displ_dp,(nproc))
     pos(1)=position;pos(2)=buf_int_size;pos(3)=buf_dp_size
     call MPI_ALLGATHER(pos,3,MPI_INTEGER,pos_all,3,MPI_INTEGER,comm,ier)
     ii=1
     do iproc=1,nproc
       counts(iproc)=pos_all(ii);ii=ii+1
       buf_int_size1(iproc)=pos_all(ii);ii=ii+1
       buf_dp_size1(iproc)=pos_all(ii);ii=ii+1
     end do

     displ(1)=0 ; displ_int(1)=0 ; displ_dp(1)=0
     do iproc=2,nproc
       displ(iproc)=displ(iproc-1)+counts(iproc-1)
       displ_int(iproc)=displ_int(iproc-1)+buf_int_size1(iproc-1)
       displ_dp(iproc)=displ_dp(iproc-1)+buf_dp_size1(iproc-1)
     end do

     totalbufcount=displ(nproc)+counts(nproc)
     ABI_STAT_MALLOC(buf_pack_tot,(totalbufcount), ier)
     if (ier/= 0) call xmpi_abort(msg='error allocating buf_pack_tot in xmpi_gatherv')
     buf_int_size_all=sum(buf_int_size1)
     buf_dp_size_all=sum(buf_dp_size1)

     if (me==root) then
       ABI_STAT_MALLOC(buf_int_all,(buf_int_size_all), ier)
       if (ier/= 0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_gatherv')
       ABI_STAT_MALLOC(buf_dp_all,(buf_dp_size_all), ier)
       if (ier/= 0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_gatherv')
     else
       ABI_STAT_MALLOC(buf_int_all,(1), ier)
       if (ier/= 0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_gatherv')
       ABI_STAT_MALLOC(buf_dp_all,(1), ier)
       if (ier/= 0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_gatherv')
     end if

!  Gather all packed messages
     call MPI_GATHERV(buf_pack,position,MPI_PACKED,buf_pack_tot,counts,displ,MPI_PACKED,root,comm,ier)
     if (me==root) then
       position=0
       do iproc=1,nproc
         lg_int=buf_int_size1(iproc); lg_dp=buf_dp_size1(iproc)
         istart_int=displ_int(iproc); istart_dp=displ_dp(iproc)
         outbuf_int=>buf_int_all(istart_int+1:istart_int+lg_int)
         call MPI_UNPACK(buf_pack_tot,totalbufcount,position, outbuf_int, &
&         lg_int, MPI_INTEGER,comm,ier)
         outbuf_dp=>buf_dp_all(istart_dp+1:istart_dp+lg_dp)
         call MPI_UNPACK(buf_pack_tot,totalbufcount,position,outbuf_dp, &
&         lg_dp, MPI_DOUBLE_PRECISION,comm,ier)
       end do
     end if

!  Release the memory
     ABI_FREE(pos_all)
     ABI_FREE(counts)
     ABI_FREE(buf_int_size1)
     ABI_FREE(buf_dp_size1)
     ABI_FREE(displ)
     ABI_FREE(displ_int)
     ABI_FREE(displ_dp)
     ABI_FREE(buf_pack_tot)
     ABI_FREE(buf_pack)

   end if
 else if (comm == MPI_COMM_SELF) then
#endif

!Sequential version
   ABI_STAT_MALLOC(buf_int_all,(buf_int_size), ier)
   if (ier/= 0) call xmpi_abort(msg='error allocating buf_int_all in xmpi_gatherv')
   ABI_STAT_MALLOC(buf_dp_all,(buf_dp_size), ier)
   if (ier/= 0) call xmpi_abort(msg='error allocating buf_dp_all in xmpi_gatherv')
   buf_int_all(:)=buf_int(:)
   buf_dp_all(:)=buf_dp(:)
   buf_int_size_all=buf_int_size
   buf_dp_size_all=buf_dp_size

#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_gatherv_int1_dp1
!!***

!!****f* ABINIT/xmpi_gatherv_int2d
!! NAME
!!  xmpi_gatherv_int2d
!!
!! FUNCTION
!!  This module contains functions that calls MPI routine,
!!  if we compile the code using the MPI CPP flags.
!!  xmpi_gatherv is the generic function.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  root= rank of receiving process
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! SOURCE

subroutine xmpi_gatherv_int2d(xval,nelem,recvbuf,recvcounts,displs,root,comm,ier)

!Arguments-------------------------
 integer, DEV_CONTARRD intent(in) :: xval(:,:)
 integer, DEV_CONTARRD intent(inout) :: recvbuf(:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,root,comm
 integer,intent(out) :: ier

!Local variables--------------
 integer :: cc,dd,sz1

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_gatherV(xval,nelem,MPI_INTEGER,recvbuf,recvcounts,displs,&
&   MPI_INTEGER,root,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz1=size(xval,1)
   dd=0;if (size(displs)>0) dd=displs(1)/sz1
   cc=size(xval,2);if (size(recvcounts)>0) cc=recvcounts(1)/sz1
   recvbuf(:,dd+1:dd+cc)=xval(:,1:cc)
#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_gatherv_int2d
!!***

!!****f* ABINIT/xmpi_gatherv_dp
!! NAME
!!  xmpi_gatherv_dp
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: one-dimensional double precision arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  root= rank of receiving process
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! SOURCE

subroutine xmpi_gatherv_dp(xval,nelem,recvbuf,recvcounts,displs,root,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:)
 real(dp), DEV_CONTARRD intent(inout) :: recvbuf(:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,root,comm
 integer,intent(out) :: ier

!Local variables--------------
 integer :: cc,dd

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_gatherV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,root,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   dd=0;if (size(displs)>0) dd=displs(1)
   cc=size(xval);if (size(recvcounts)>0) cc=recvcounts(1)
   recvbuf(dd+1:dd+cc)=xval(1:cc)
#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_gatherv_dp
!!***

!!****f* ABINIT/xmpi_gatherv_dp2d
!! NAME
!!  xmpi_gatherv_dp2d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision two-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  root= rank of receiving process
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! SOURCE

subroutine xmpi_gatherv_dp2d(xval,nelem,recvbuf,recvcounts,displs,root,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:)
 real(dp), DEV_CONTARRD intent(inout) :: recvbuf(:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,root,comm
 integer,intent(out) :: ier

!Local variables--------------
 integer :: cc,dd,sz1

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_gatherV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,root,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz1=size(xval,1)
   dd=0;if (size(displs)>0) dd=displs(1)/sz1
   cc=size(xval,2);if (size(recvcounts)>0) cc=recvcounts(1)/sz1
   recvbuf(:,dd+1:dd+cc)=xval(:,1:cc)
#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_gatherv_dp2d
!!***

!!****f* ABINIT/xmpi_gatherv_dp3d
!! NAME
!!  xmpi_gatherv_dp3d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision three-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  root= rank of receiving process
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! SOURCE

subroutine xmpi_gatherv_dp3d(xval,nelem,recvbuf,recvcounts,displs,root,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:,:)
 real(dp), DEV_CONTARRD intent(inout)   :: recvbuf(:,:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,root,comm
 integer,intent(out) :: ier

!Local variables--------------
 integer :: cc,dd,sz12

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_gatherV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,root,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz12=size(xval,1)*size(xval,2)
   dd=0;if (size(displs)>0) dd=displs(1)/sz12
   cc=size(xval,3);if (size(recvcounts)>0) cc=recvcounts(1)/sz12
   recvbuf(:,:,dd+1:dd+cc)=xval(:,:,1:cc)
#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_gatherv_dp3d
!!***

!!****f* ABINIT/xmpi_gatherv_dp4d
!! NAME
!!  xmpi_gatherv_dp4d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision four-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  root= rank of receiving process
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! SOURCE

subroutine xmpi_gatherv_dp4d(xval,nelem,recvbuf,recvcounts,displs,root,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:,:,:)
 real(dp), DEV_CONTARRD intent(inout)   :: recvbuf(:,:,:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,root,comm
 integer,intent(out) :: ier

!Local variables-------------------
 integer :: cc,dd,sz123

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_gatherV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,root,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz123=size(xval,1)*size(xval,2)*size(xval,3)
   dd=0;if (size(displs)>0) dd=displs(1)/sz123
   cc=size(xval,4);if (size(recvcounts)>0) cc=recvcounts(1)/sz123
   recvbuf(:,:,:,dd+1:dd+cc)=xval(:,:,:,1:cc)
#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_gatherv_dp4d
!!***

!!****f* ABINIT/xmpi_gatherv_dp5d
!! NAME
!!  xmpi_gatherv_dp5d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision four-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  root= rank of receiving process
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! SOURCE

subroutine xmpi_gatherv_dp5d(xval,nelem,recvbuf,recvcounts,displs,root,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:,:,:,:)
 real(dp), DEV_CONTARRD intent(inout)   :: recvbuf(:,:,:,:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,root,comm
 integer,intent(out) :: ier

!Local variables-------------------
 integer :: cc,dd,sz1234

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_gatherV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,root,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz1234=size(xval,1)*size(xval,2)*size(xval,3)*size(xval,4)
   dd=0;if (size(displs)>0) dd=displs(1)/sz1234
   cc=size(xval,5);if (size(recvcounts)>0) cc=recvcounts(1)/sz1234
   recvbuf(:,:,:,:,dd+1:dd+cc)=xval(:,:,:,:,1:cc)
#if defined HAVE_MPI
 end if
#endif
!!***

end subroutine xmpi_gatherv_dp5d

!!****f* ABINIT/xmpi_gatherv_dp6d
!! NAME
!!  xmpi_gatherv_dp6d
!!
!! FUNCTION
!!  Gathers data from all tasks and delivers it to all.
!!  Target: double precision four-dimensional arrays.
!!
!! INPUTS
!!  xval= buffer array
!!  recvcounts= number of received elements
!!  displs= relative offsets for incoming data
!!  nelem= number of elements
!!  root= rank of receiving process
!!  comm= MPI communicator
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  recvbuf= received buffer
!!
!! SOURCE

subroutine xmpi_gatherv_dp6d(xval,nelem,recvbuf,recvcounts,displs,root,comm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(in) :: xval(:,:,:,:,:,:)
 real(dp), DEV_CONTARRD intent(inout)   :: recvbuf(:,:,:,:,:,:)
 integer, DEV_CONTARRD intent(in) :: recvcounts(:),displs(:)
 integer,intent(in) :: nelem,root,comm
 integer,intent(out) :: ier

!Local variables-------------------
 integer :: cc,dd,sz12345

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (comm /= MPI_COMM_SELF .and. comm /= MPI_COMM_NULL) then
   call MPI_gatherV(xval,nelem,MPI_DOUBLE_PRECISION,recvbuf,recvcounts,displs,&
&   MPI_DOUBLE_PRECISION,root,comm,ier)
 else if (comm == MPI_COMM_SELF) then
#endif
   sz12345=size(xval,1)*size(xval,2)*size(xval,3)*size(xval,4)*size(xval,5)
   dd=0;if (size(displs)>0) dd=displs(1)/sz12345
   cc=size(xval,6);if (size(recvcounts)>0) cc=recvcounts(1)/sz12345
   recvbuf(:,:,:,:,:,dd+1:dd+cc)=xval(:,:,:,:,:,1:cc)
#if defined HAVE_MPI
 end if
#endif

end subroutine xmpi_gatherv_dp6d
!!***
