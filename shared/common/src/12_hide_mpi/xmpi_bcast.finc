!{\src2tex{textfont=tt}}
!!****f* ABINIT/xmpi_bcast_intv
!! NAME
!!  xmpi_bcast_intv
!!
!! FUNCTION
!!  This module contains functions that calls MPI routine,
!!  if we compile the code using the MPI CPP flags.
!!  xmpi_bcast is the generic function.
!!
!! COPYRIGHT
!!  Copyright (C) 2001-2020 ABINIT group (Rshaltaf,AR,XG)
!!  This file is distributed under the terms of the
!!  GNU General Public License, see ~ABINIT/COPYING
!!  or http://www.gnu.org/copyleft/gpl.txt .
!!
!! SOURCE

subroutine xmpi_bcast_intv(xval,master,spaceComm,ier)

!Arguments-------------------------
 integer,intent(inout) :: xval
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   call MPI_BCAST(xval,1,MPI_INTEGER,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_intv
!!***

!!****f* ABINIT/xmpi_bcast_int1d
!! NAME
!!  xmpi_bcast_int1d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: one-dimensional integer arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! PARENTS
!!
!! CHILDREN
!!      mpi_bcast
!!
!! SOURCE
subroutine xmpi_bcast_int1d(xval,master,spaceComm,ier)

!Arguments ------------------------------------
 integer, DEV_CONTARRD intent(inout) :: xval(:)
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

!Local variables-------------------------------
 integer :: n

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n=size(xval)
   call MPI_BCAST(xval,n,MPI_INTEGER,master,spaceComm,ier)
 end if
#endif
end subroutine xmpi_bcast_int1d
!!***

!!****f* ABINIT/xmpi_bcast_int2d
!! NAME
!!  xmpi_bcast_int2d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: two-dimensional integer arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! PARENTS
!!
!! CHILDREN
!!      mpi_bcast
!!
!! SOURCE

subroutine xmpi_bcast_int2d(xval,master,spaceComm,ier)

!Arguments-------------------------
 integer, DEV_CONTARRD intent(inout) :: xval(:,:)
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2,MPI_INTEGER,master,spaceComm,ier)
   else
     call xmpi_largetype_create(ntot,MPI_INTEGER,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,1,my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if

 end if
#endif

end subroutine xmpi_bcast_int2d
!!***

!!****f* ABINIT/xmpi_bcast_int3d
!! NAME
!!  xmpi_bcast_int3d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: three-dimensional integer arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! PARENTS
!!
!! CHILDREN
!!      mpi_bcast
!!
!! SOURCE
subroutine xmpi_bcast_int3d(xval,master,spaceComm,ier)

!Arguments-------------------------
 integer, DEV_CONTARRD intent(inout) :: xval(:,:,:)
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

!Local variables-------------------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2*n3,MPI_INTEGER,master,spaceComm,ier)
   else
     nn=int(n1*n2,kind=int64);if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_INTEGER,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_int3d
!!***

!!****f* ABINIT/xmpi_bcast_dpv
!! NAME
!!  xmpi_bcast_dpv
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: scalar double precisions.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! PARENTS
!!
!! CHILDREN
!!      mpi_bcast
!!
!! SOURCE

subroutine xmpi_bcast_dpv(xval,master,spaceComm,ier)

!Arguments ------------------------------------
 real(dp),intent(inout) :: xval
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier
!Local variables-------------------------------

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   call MPI_BCAST(xval,1,MPI_DOUBLE_PRECISION,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_dpv
!!***

!!****f* ABINIT/xmpi_bcast_dp1d
!! NAME
!!  xmpi_bcast_dp1d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: double precision one-dimensional arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! PARENTS
!!
!! CHILDREN
!!      mpi_bcast
!!
!! SOURCE
subroutine xmpi_bcast_dp1d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: n
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n=size(xval,dim=1)
   call MPI_BCAST(xval,n,MPI_DOUBLE_PRECISION,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_dp1d
!!***

!!****f* ABINIT/xmpi_bcast_dp2d
!! NAME
!!  xmpi_bcast_dp2d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: double precision two-dimensional arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dp2d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2,MPI_DOUBLE_PRECISION,master,spaceComm,ier)
   else
     call xmpi_largetype_create(ntot,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,1,my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_dp2d
!!***

!!****f* ABINIT/xmpi_bcast_dp3d
!! NAME
!!  xmpi_bcast_dp3d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: double precision three-dimensional arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dp3d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2*n3,MPI_DOUBLE_PRECISION,master,spaceComm,ier)
   else
     nn=int(n1*n2,kind=int64);if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_dp3d
!!***

!!****f* ABINIT/xmpi_bcast_dp4d
!! NAME
!!  xmpi_bcast_dp4d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: double precision four-dimensional arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dp4d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)
   n4=size(xval,dim=4)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3*n4,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2*n3*n4,MPI_DOUBLE_PRECISION,master,spaceComm,ier)
   else
     nn=int(n1*n2*n3,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_dp4d
!!***

!!****f* ABINIT/xmpi_bcast_dp5d
!! NAME
!!  xmpi_bcast_dp5d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: double precision five-dimensional arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dp5d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4,n5
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)
   n4=size(xval,dim=4)
   n5=size(xval,dim=5)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3*n4*n5,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2*n3*n4*n5,MPI_DOUBLE_PRECISION,master,spaceComm,ier)
   else
     nn=int(n1*n2*n3*n4,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2*n3,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_dp5d
!!***

!!****f* ABINIT/xmpi_bcast_dp6d
!! NAME
!!  xmpi_bcast_dp6d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: double precision six-dimensional arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dp6d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(dp), DEV_CONTARRD intent(inout) :: xval(:,:,:,:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4,n5,n6
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)
   n4=size(xval,dim=4)
   n5=size(xval,dim=5)
   n6=size(xval,dim=6)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3*n4*n5*n6,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2*n3*n4*n5*n6,MPI_DOUBLE_PRECISION,master,spaceComm,ier)
   else
     nn=int(n1*n2*n3*n4*n5,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2*n3*n4,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2*n3,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_DOUBLE_PRECISION,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_dp6d
!!***

!!****f* ABINIT/xmpi_bcast_spv
!! NAME
!!  xmpi_bcast_spv
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: scalar single precisions.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_spv(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(sp),intent(inout) :: xval
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   call MPI_BCAST(xval,1,MPI_REAL,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_spv
!!***

!!****f* ABINIT/xmpi_bcast_sp1d
!! NAME
!!  xmpi_bcast_sp1d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: one-dimensional single precision arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_sp1d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(sp), DEV_CONTARRD intent(inout) :: xval(:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: n
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n=size(xval,dim=1)
   call MPI_BCAST(xval,n,MPI_REAL,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_sp1d
!!***

!!****f* ABINIT/xmpi_bcast_sp2d
!! NAME
!!  xmpi_bcast_sp2d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: two-dimensional single precision arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_sp2d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(sp), DEV_CONTARRD intent(inout) :: xval(:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2,MPI_REAL,master,spaceComm,ier)
   else
     call xmpi_largetype_create(ntot,MPI_REAL,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,1,my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_sp2d
!!***

!!****f* ABINIT/xmpi_bcast_sp3d
!! NAME
!!  xmpi_bcast_sp3d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: three-dimensional single precision arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_sp3d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(sp), DEV_CONTARRD intent(inout) :: xval(:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
   else
     nn=int(n1*n2,kind=int64);if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_REAL,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_sp3d
!!***

!!****f* ABINIT/xmpi_bcast_sp4d
!! NAME
!!  xmpi_bcast_sp4d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: four-dimensional single precision arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_sp4d(xval,master,spaceComm,ier)

!Arguments-------------------------
 real(sp), DEV_CONTARRD intent(inout) :: xval(:,:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)
   n4=size(xval,dim=4)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3*n4,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
   else
     nn=int(n1*n2*n3,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_REAL,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_sp4d
!!***

!!****f* ABINIT/xmpi_bcast_cplxv
!! NAME
!!  xmpi_bcast_cplxv
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: scalar complexs.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! PARENTS
!!
!! CHILDREN
!!      mpi_bcast
!!
!! SOURCE
subroutine xmpi_bcast_cplxv(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(spc),intent(inout) :: xval
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   call MPI_BCAST(xval,1,MPI_COMPLEX,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_cplxv
!!***

!!****f* ABINIT/xmpi_bcast_cplx1d
!! NAME
!!  xmpi_bcast_cplx1d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: one-dimensional complex arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! PARENTS
!!
!! CHILDREN
!!      mpi_bcast
!!
!! SOURCE
subroutine xmpi_bcast_cplx1d(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: n
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n=size(xval(:))
   call MPI_BCAST(xval,n,MPI_COMPLEX,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_cplx1d
!!***

!!****f* ABINIT/xmpi_bcast_cplx2d
!! NAME
!!  xmpi_bcast_cplx2d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: two-dimensional complex arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_cplx2d(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2,MPI_COMPLEX,master,spaceComm,ier)
   else
     call xmpi_largetype_create(ntot,MPI_COMPLEX,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,1,my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_cplx2d
!!***

!!****f* ABINIT/xmpi_bcast_cplx3d
!! NAME
!!  xmpi_bcast_cplx3d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: three-dimensional complex arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_cplx3d(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
   else
     nn=int(n1*n2,kind=int64);if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_COMPLEX,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_cplx3d
!!***

!!****f* ABINIT/xmpi_bcast_cplx4d
!! NAME
!!  xmpi_bcast_cplx4d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: four-dimensional complex arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE
subroutine xmpi_bcast_cplx4d(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(spc), DEV_CONTARRD intent(inout) :: xval(:,:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)
   n4=size(xval,dim=4)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3*n4,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2*n3*n4,MPI_COMPLEX,master,spaceComm,ier)
   else
     nn=int(n1*n2*n3,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_COMPLEX,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_cplx4d
!!***

!!****f* ABINIT/xmpi_bcast_dcv
!! NAME
!!  xmpi_bcast_dcv
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: scalar double complexs.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dcv(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(dpc),intent(inout):: xval
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   call MPI_BCAST(xval,1,MPI_DOUBLE_COMPLEX,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_dcv
!!***

!!****f* ABINIT/xmpi_bcast_dc1d
!! NAME
!!  xmpi_bcast_dc1d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: one-dimensional double complex arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dc1d(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout):: xval(:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: n
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n=size(xval(:))
   call MPI_BCAST(xval,n,MPI_DOUBLE_COMPLEX,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_dc1d
!!***

!!****f* ABINIT/xmpi_bcast_dc2d
!! NAME
!!  xmpi_bcast_dc2d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: two-dimensional double complex arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!!
!! SOURCE

subroutine xmpi_bcast_dc2d(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout):: xval(:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2
 integer(kind=int64) :: ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2,MPI_DOUBLE_COMPLEX,master,spaceComm,ier)
   else
     call xmpi_largetype_create(ntot,MPI_DOUBLE_COMPLEX,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,1,my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_dc2d
!!***

!!****f* ABINIT/xmpi_bcast_dc3d
!! NAME
!!  xmpi_bcast_dc3d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: three-dimensional double complex arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dc3d(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout):: xval(:,:,:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2*n3,MPI_DOUBLE_COMPLEX,master,spaceComm,ier)
   else
     nn=int(n1*n2,kind=int64);if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_DOUBLE_COMPLEX,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_dc3d
!!***

!!****f* ABINIT/xmpi_bcast_dc4d
!! NAME
!!  xmpi_bcast_dc4d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: four-dimensional complex arrays in double precision.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_dc4d(xval,master,spaceComm,ier)

!Arguments-------------------------
 complex(dpc), DEV_CONTARRD intent(inout) :: xval(:,:,:,:)
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: my_dt,my_op,n1,n2,n3,n4
 integer(kind=int64) :: nn,ntot
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   n1=size(xval,dim=1)
   n2=size(xval,dim=2)
   n3=size(xval,dim=3)
   n4=size(xval,dim=4)

   !This product of dimensions can be greater than a 32bit integer
   !We use a INT64 to store it. If it is too large, we switch to an
   !alternate routine because MPI<4 doesnt handle 64 bit counts.
   ntot=int(n1*n2*n3*n4,kind=int64)

   if (ntot<=xmpi_maxint32_64) then
     call MPI_BCAST(xval,n1*n2*n3*n4,MPI_DOUBLE_COMPLEX,master,spaceComm,ier)
   else
     nn=int(n1*n2*n3,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1*n2,kind=int64)
     if (nn>huge(0_int32)) nn=int(n1,kind=int64)
     call xmpi_largetype_create(ntot/nn,MPI_DOUBLE_COMPLEX,my_dt,my_op,MPI_OP_NULL)
     call MPI_BCAST(xval,int(nn,kind=int32),my_dt,master,spaceComm,ier)
     call xmpi_largetype_free(my_dt,my_op)
   end if
 end if
#endif

end subroutine xmpi_bcast_dc4d
!!***

!!****f* ABINIT/xmpi_bcast_ch0d
!! NAME
!!  xmpi_bcast_ch0d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: character strings.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_ch0d(xval,master,spaceComm,ier)

!Arguments-------------------------
 character(len=*),intent(inout) :: xval
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

!Local variables-------------------------------
#if defined HAVE_MPI
 integer :: nch,rank
#endif

!*************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   call MPI_COMM_RANK(spaceComm,rank,ier)
   if (rank==master) nch=len_trim(xval)
   call MPI_BCAST(nch,1,MPI_INTEGER,master,spaceComm,ier)
   call MPI_BCAST(xval,nch,MPI_CHARACTER,master,spaceComm,ier)
   if (rank/=master) xval(nch+1:)=''
 end if
#endif

end subroutine xmpi_bcast_ch0d
!!***

!!****f* ABINIT/xmpi_bcast_ch1d
!! NAME
!!  xmpi_bcast_ch1d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: one-dimensional array of character stringss.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE
subroutine xmpi_bcast_ch1d(xval,master,spaceComm,ier)

!Arguments-------------------------
 Character(len=*), DEV_CONTARRD intent(inout) :: xval(:)
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

!Local variables-------------------------------
#if defined HAVE_MPI
 integer :: ii,nch
#endif

!*************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   nch=0
   do ii=1,size(xval)
     nch=nch+len(xval(ii))
   end do
   call MPI_BCAST(xval,nch,MPI_CHARACTER,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_ch1d
!!***


!!****f* ABINIT/xmpi_bcast_log0d
!! NAME
!!  xmpi_bcast_log0d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: logical scalar
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SOURCE

subroutine xmpi_bcast_log0d(xval,master,spaceComm,ier)

!Arguments-------------------------
 logical,intent(inout) :: xval
 integer,intent(in) :: spaceComm,master
 integer,intent(out) :: ier

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   call MPI_BCAST(xval,1,MPI_LOGICAL,master,spaceComm,ier)
 end if
#endif

end subroutine xmpi_bcast_log0d
!!***

!!****f* ABINIT/xmpi_bcast_coeffi2_1d
!! NAME
!!  xmpi_bcast_coeffi2_1d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: type(coeffi2) 1D-arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_coeffi2_1d(xval,master,spaceComm,ier)

!Arguments-------------------------
 type(coeffi2_type), intent(inout) :: xval(:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: ii,jj,kk,me,n0,n1,n2,siztot
 integer,allocatable :: mpibuf(:),siz(:,:)
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   me=xmpi_comm_rank(spaceComm)

!  Broadcast xval%value sizes
   n0=size(xval)
   ABI_STAT_ALLOCATE(siz,(2,n0), ier)
   if (ier/= 0) call xmpi_abort(msg='error allocating siz in xmpi_bcast')
   if (me==master) then
     do ii=1,n0
       siz(1,ii)=size(xval(ii)%value,1)
       siz(2,ii)=size(xval(ii)%value,2)
     end do
   end if
   call MPI_BCAST(siz,2*n0,MPI_INTEGER,master,spaceComm,ier)
   siztot=0
   do ii=1,n0
     siztot=siztot+siz(1,ii)*siz(2,ii)
   end do

!  Fill in the buffer
   ABI_STAT_ALLOCATE(mpibuf,(siztot), ier)
   if (ier/= 0) call xmpi_abort(msg='error allocating mpibuf in xmpi_bcast')
   if (me==master) then
     jj=0
     do ii=1,n0
       n1=siz(1,ii);n2=siz(2,ii)
       do kk=1,n2
         mpibuf(jj+1:jj+n1)=xval(ii)%value(1:n1,kk)
         jj=jj+n1
       end do
     end do
   end if

!  Broadcast the data
   call MPI_BCAST(mpibuf,siztot,MPI_INTEGER,master,spaceComm,ier)

!  Retrieve the buffer
   jj=0
   do ii=1,n0
     n1=siz(1,ii);n2=siz(2,ii)
     if (.not.allocated(xval(ii)%value)) then
       ABI_STAT_ALLOCATE(xval(ii)%value,(n1,n2), ier)
       if (ier/= 0) call xmpi_abort(msg='error allocating xval%value in xmpi_bcast')
     end if
     do kk=1,n2
       xval(ii)%value(1:n1,kk)=mpibuf(jj+1:jj+n1)
       jj=jj+n1
     end do
   end do
   ABI_DEALLOCATE(siz)
   ABI_DEALLOCATE(mpibuf)

 end if
#endif

end subroutine xmpi_bcast_coeffi2_1d
!!***

!!****f* ABINIT/xmpi_bcast_coeff2_1d
!! NAME
!!  xmpi_bcast_coeff2_1d
!!
!! FUNCTION
!!  Broadcasts data from master to slaves.
!!  Target: type(coeff2) 1D-arrays.
!!
!! INPUTS
!!  spaceComm= MPI communicator
!!  master= master MPI node
!!
!! OUTPUT
!!  ier= exit status, a non-zero value meaning there is an error
!!
!! SIDE EFFECTS
!!  xval= buffer array
!!
!! SOURCE

subroutine xmpi_bcast_coeff2_1d(xval,master,spaceComm,ier)

!Arguments-------------------------
 type(coeff2_type), intent(inout) :: xval(:)
 integer ,intent(in) :: spaceComm,master
 integer ,intent(out) :: ier

!Local variables-------------------
#if defined HAVE_MPI
 integer :: ii,jj,kk,me,n0,n1,n2,siztot
 integer,allocatable :: siz(:,:)
 real(dp),allocatable :: mpibuf(:)
#endif

! *************************************************************************

 ier=0
#if defined HAVE_MPI
 if (spaceComm /= MPI_COMM_SELF .and. spaceComm /= MPI_COMM_NULL) then
   me=xmpi_comm_rank(spaceComm)

!  Broadcast xval%value sizes
   n0=size(xval)
   ABI_STAT_ALLOCATE(siz,(2,n0), ier)
   if (ier/= 0) call xmpi_abort(msg='error allocating siz in xmpi_bcast')
   if (me==master) then
     do ii=1,n0
       siz(1,ii)=size(xval(ii)%value,1)
       siz(2,ii)=size(xval(ii)%value,2)
     end do
   end if
   call MPI_BCAST(siz,2*n0,MPI_INTEGER,master,spaceComm,ier)
   siztot=0
   do ii=1,n0
     siztot=siztot+siz(1,ii)*siz(2,ii)
   end do

!  Fill in the buffer
   ABI_STAT_ALLOCATE(mpibuf,(siztot), ier)
   if (ier/= 0) call xmpi_abort(msg='error allocating mpibuf in xmpi_bcast')
   if (me==master) then
     jj=0
     do ii=1,n0
       n1=siz(1,ii);n2=siz(2,ii)
       do kk=1,n2
         mpibuf(jj+1:jj+n1)=xval(ii)%value(1:n1,kk)
         jj=jj+n1
       end do
     end do
   end if

!  Broadcast the data
   call MPI_BCAST(mpibuf,siztot,MPI_DOUBLE_PRECISION,master,spaceComm,ier)

!  Retrieve the buffer
   jj=0
   do ii=1,n0
     n1=siz(1,ii);n2=siz(2,ii)
     if (.not.allocated(xval(ii)%value)) then
       ABI_STAT_ALLOCATE(xval(ii)%value,(n1,n2), ier)
       if (ier/= 0) call xmpi_abort(msg='error allocating xval%value in xmpi_bcast')
     end if
     do kk=1,n2
       xval(ii)%value(1:n1,kk)=mpibuf(jj+1:jj+n1)
       jj=jj+n1
     end do
   end do
   ABI_DEALLOCATE(siz)
   ABI_DEALLOCATE(mpibuf)

 end if
#endif

end subroutine xmpi_bcast_coeff2_1d
!!***

