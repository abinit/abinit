---
authors: MG
---

# How to compile ABINIT

This tutorial explains how to compile ABINIT including the required external dependencies
without relying on pre-compiled libraries, package managers and root privileges.
You will learn how to use the standard **configure** and **make** Linux tools
to build and install your own software stack including the MPI library and the associated
*mpif90* and *mpicc* wrappers required to compile parallel MPI applications.

It is assumed that you already have a standard Unix-like installation
that provides the basic tools needed to build software from source (Fortran/C compilers and *make*).
The changes required for MacOsX are briefly mentioned if needed.
Windows users should install [cygwin](https://cygwin.com/index.html) that
provides a POSIX-compatible environment
or, alternatively, use a [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about).
Note that the procedure described in this tutorial has been tested with Linux/MacOsX hence
feedback and suggestions from Windows users are welcome.

!!! tip

    In the last part of the tutorial, we discuss more advanced topics such as using modules in supercomputing centers,
    compiling and linking with the intel compiler and the MKL library as well as OpenMP threads.
    You may want to jump directly to this section if you are already familiar with software compilation.

In the following, we will make extensive use of the bash shell hence familiarity with the terminal is assumed.
For a quick introduction to the command line, please consult
this [Ubuntu tutorial](https://ubuntu.com/tutorials/command-line-for-beginners#1-overview).
If this is the first time you use the **configure && make** approach to build software,
we **strongly** recommended to read this
[guide](https://www.codecoffee.com/software-installation-configure-make-install/)
before proceeding with the next steps.

If you are not interested in compiling ABINIT from source, you may want to consider the following alternatives:

* Compilation with external libraries provided by apt-get based Linux distributions.
  More info available [here](INSTALL_Ubuntu)

* Compilation with external libraries on Fedora/RHEL/CentOS Linux distributions.
  More info available [here](INSTALL_CentOS)

* Homebrew bottles or macports for MacOsX users.
  More info available [here](INSTALL_MacOS).

* Compiling Abinit using the internal fallbacks and the *build-abinit-fallbacks.sh* script
  automatically generated by *configure* if the mandatory dependencies are not found.

* Using precompiled binaries provided by conda-forge (for Linux and MacOsX users).

Before starting, it is also worth reading this document prepared by Marc Torrent
that introduces important concepts as well as
a detailed description of the configuration options supported by the ABINIT build system
The examples are for Abinit v8 and some options should be changed to be compatible
with the build system of version 9 still the document is a valuable source of information.

<embed src="https://school2019.abinit.org/images/lectures/abischool2019_installing_abinit_lecture.pdf"
type="application/pdf" width="100%" height="480px">


!!! important

    The aim of this tutorial is to teach you how to compile code from source but we cannot guarantee
    that these recipes will work out of the box on every possible architecture.
    We will do our best to explain how to setup your environment and how to avoid the typical pitfalls but
    we cannot cover all the possible cases.
    Fortunately, the internet provides lots of resources.
    Search engines and stackoverflow are your best friends and in some cases one can find the solution
    by just copying the error message in the search bar.
    For more complicated issues, you can ask for help on the [Abinit forum](https://forum.abinit.org)
    or contact the sysadmin of your cluster but remember to provide enough information about your system
    and the issue to help the developers.

## Getting started

Since ABINIT is written in Fortran, we need a **recent** Fortran compiler
that supports the **F2003 specifications** as well as a C compiler.
At the time of writing ( |today| ), the C++ compiler is optional and required only for advanced features
such as the interface with the TRIQS library that, however, is not discussed in this tutorial.

In what follows, we will be focusing on the GNU toolchain i.e. *gcc* for C and *gfortran* for Fortran.
These "sequential" compilers are adequate if you don't need to compile parallel MPI applications.
The compilation of MPI code, indeed, requires the installation of **additional libraries**
and **specialized wrappers** (*mpif90*, *mpicc*) replacing the "sequential" compilers.
This very important scenario is covered in more details in the next sections.
For the time being, we mainly focus on the compilation of sequential applications/libraries.

First of all, let's make sure the **gfortran** compiler is installed on your machine
by issuing in the terminal the following command:

```sh
which gfortran
/usr/bin/gfortran
```

!!! tip

    The **which** command, returns the absolute path of the executable.
    This Unix tool is extremely useful to pinpoint possible problems and we will use it
    a lot in the rest of this tutorial.

In our case, we are lucky that the Fortran compiler is already installed in */usr/bin* and we can immediately
use it to build our software stack.
If *gfortran* is not installed, you may want to use the package manager provided by your
Linux distribution to install it.
On Ubuntu, for instance, use:

```sh
sudo apt-get install gfortran
```

To get the version of the compiler, use the `--version` option:

```sh
gfortran --version
GNU Fortran (GCC) 5.3.1 20160406 (Red Hat 5.3.1-6)
Copyright (C) 2015 Free Software Foundation, Inc.
```

Now let's check whether **make** is already installed using:

```sh
which make
/usr/bin/make
```

Hopefully, the C compiler *gcc* is already installed on your machine.
At this point, we have all the basic building blocks needed to compile ABINIT from source and we
can proceed with the next steps.

!!! tip

    Life get hard if you are a Mac-OsX user as Apple does not officially
    support Fortran so you will need to install gfortran and gcc either via
    [homebrew](https://brew.sh/) or [macport](https://www.macports.org/).
    Alternatively, one can install gfortran using one of the standalone DMG installers
    provided by the [gfortran-for-macOS project](https://github.com/fxcoudert/gfortran-for-macOS/releases).
    Note also that MaxOsX users will need to install **make** via [Xcode](https://developer.apple.com/xcode/).
    More info are available [here](INSTALL_MacOS).

## How to compile BLAS and LAPACK

BLAS and LAPACK represent the workhorse of many scientific codes and an optimized implementation
is crucial for achieving **good performance**.
In principle this step can be skipped as any decent Linux distribution already provides
pre-compiled versions but, as already mentioned in the introduction, we are geeks and we
prefer to compile everything from source.
Moreover the compilation of BLAS/LAPACK represents an excellent exercise
that gives us the opportunity to discuss some basic concepts that
will reveal useful in the other parts of this tutorial.

First of all, let's create a new directory inside your `$HOME` (let's call it **local**) using the command:

```sh
cd $HOME && mkdir local
```

!!! tip

    $HOME is a standard shell variable that stores the absolute path to your home directory.
    Use:

    ```sh
    echo My home directory is $HOME
    ```

    to print the value of the variable.

    The **&&** syntax is used to chain commands together, such that the next command is executed if and only
    if the preceding command exited without errors (or, more accurately, exits with a return code of 0).
    We wil use this trick a lot in the other examples to reduce the number of lines we have to type
    in the terminal so that one can easily cut and paste the examples in the terminal.


Now create the `src` subdirectory inside $HOME/local with:

```sh
cd $HOME/local && mkdir src && cd src
```

The *src* directory will be used to store the packages with the source files and compile code,
whereas executables and libraries will be installed in `$HOME/local/bin` and `$HOME/local/lib`, respectively.
We use `$HOME/local/` because we are working as **normal users** and we cannot install software
in `/usr/local` where root privileges are required and `sudo make install` is needed.
Moreover working inside `$HOME/local` allows us to keep our software stack well separated
from the libraries installed by our Linux distribution so that we can easily test new libraries and/or
different versions without affecting the software stack installed by our distribution.

Now download the tarball from the [openblas website](https://www.openblas.net/) with:

```sh
wget https://github.com/xianyi/OpenBLAS/archive/v0.3.7.tar.gz
```

If *wget* is not available, use *curl* with the `-o` option to specify the name of the output file as in:

```sh
curl https://github.com/xianyi/OpenBLAS/archive/v0.3.7.tar.gz -o v0.3.7.tar.gz
```

!!! tip

    To get the URL associated to a HTML link inside the browser, hover the mouse pointer over the link,
    press the right mouse button and then select `Copy Link Address` to copy the link to the system clipboard.
    Then paste the text in the terminal by selecting the `Copy` action in the menu
    activated by clicking on the right button.
    Alternatively, one can press the central button (mouse wheel) or use CMD + V on MacOsX.
    This trick is quite handy to fetch tarballs directly from the terminal.


Uncompress the tarball with:

```sh
tar -xvf v0.3.7.tar.gz
```

then `cd` to the directory with:

```sh
cd OpenBLAS-0.3.7
```

and execute

```sh
make -j2 USE_THREAD=0 USE_LOCKING=1
```

to build the single thread version.
By default, *openblas* activates threads (see [FAQ page](https://github.com/xianyi/OpenBLAS/wiki/Faq#multi-threaded))
but in our case we prefer to use the sequential version as Abinit is mainly optimized for MPI.
The `-j2` option tells *make* to use 2 processes to build the code in order to speed up the compilation.
Adjust this value according to the number of **physical cores** available on your machine.

At the end of the compilation, you should get the following output:

```md
 OpenBLAS build complete. (BLAS CBLAS LAPACK LAPACKE)

  OS               ... Linux
  Architecture     ... x86_64
  BINARY           ... 64bit
  C compiler       ... GCC  (command line : cc)
  Fortran compiler ... GFORTRAN  (command line : gfortran)
  Library Name     ... libopenblas_haswell-r0.3.7.a (Single threaded)

To install the library, you can run "make PREFIX=/path/to/your/installation install".
```

<!--
As a side note, a compilation with plain *make* would give:

```md
  Library Name     ... libopenblas_haswellp-r0.3.7.a (Multi threaded; Max num-threads is 12)
```

that indicates that the openblas library now supports threads.
-->

You may have noticed that, in this particular case, *make* is not just building the library but is also
running unit tests to validate the build.
This means that if *make* completes successfully, we can be confident that the build is OK
and we can proceed with the installation.
Other packages use a different philosophy and provide a **make check** option that should be executed after *make*
in order to run the test suite before installing the package.

To install openblas in $HOME/local, issue:

```sh
make PREFIX=$HOME/local/ install
```

At this point, we should have the following libraries **installed in $HOME/local/lib**:

```sh
ls $HOME/local/lib/libopenblas*

/home/gmatteo/local/lib/libopenblas.a     /home/gmatteo/local/lib/libopenblas_haswell-r0.3.7.a
/home/gmatteo/local/lib/libopenblas.so    /home/gmatteo/local/lib/libopenblas_haswell-r0.3.7.so
/home/gmatteo/local/lib/libopenblas.so.0
```

Files ending with `.so` are **shared libraries** (`.so` stands for shared object) whereas
`.a` files are **static libraries**.
When compiling source code that relies on external libraries, the name of the library
(without the *lib* prefix and the file extension) as well as the directory where the library is located must be passed
to the compiler that in turn will pass these parameters to the linker when building the executable.

The name of the library is usually specified with the `-l` option while the directory is given by `-L`.
According to these simple rules, in order to compile source code that uses BLAS/LAPACK routines,
one should pass to the compiler the following options:

    -L$HOME/local/lib -lopenblas

We will use a similar syntax to help the ABINIT configure script locate the external linear algebra library.

!!! important

    You may have noticed that we haven't specified the file extension in the library name.
    If both static and shared libraries are found, the linker gives preference to linking with the shared library
    unless the `-static` option is used.
    Dynamic is the default behaviour on several Linux distributions so we assume dynamic linking
    in what follows.

If you are compiling C or Fortran code requiring include files with the declaration of prototypes and the definition
of named constants, you will need to specify the location of the **include files** via the `-I` option.
In this case, the previous options should be replaced by:

```sh
-L$HOME/local/lib -lopenblas -I$HOME/local/include
```

This approach is quite common for C code where `.h` files must be included to compile properly.
It is less common for modern Fortran code in which include files are usually replaced by `.mod` files
*i.e.* module files produced by the Fortran compiler whose location is specified via the `-J` option.
Still, the `-I` option for include files is valuable also when compiling Fortran applications as important libraries
such as FFTW and MKL rely on (Fortran) include files whose location should be passed to the compiler
via `-I` instead of `-J`,
see also the official [gfortran documentation](https://gcc.gnu.org/onlinedocs/gfortran/Directory-Options.html#Directory-Options).

Don't worry if this rather technical point is not clear to you.
Any external library has its own requirements and peculiarities and the ABINIT build system provides several options
to automate the detection of external dependencies and the final linkage.
The most important thing is that you are now aware that the compilation of ABINIT requires
the correct specification of the `-L`, `-l`, `-I` and `-J` options.
We will elaborate more on this topic when we discuss the configuration options supported by the ABINIT build system.

<!--
https://gcc.gnu.org/onlinedocs/gcc/Link-Options.html

```sh
nm $HOME/local/lib/libopenblas.so
```
to list the symbols presented in the library.
-->

Since we have installed the package in a **non-standard directory** ($HOME/local),
we need to update two important shell variables: `$PATH` and `$LD_LIBRARY_PATH`.
If this is the first time you hear about $PATH and $LD_LIBRARY_PATH, please take some time to learn
about the meaning of these environment variables.
More information about `$PATH` is available [here](http://www.linfo.org/path_env_var.html).
See [this page](https://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html) for `$LD_LIBRARY_PATH`.

Add these two lines at the end of your `$HOME/.bash_profile` file

```sh
export PATH=$HOME/local/bin:$PATH

export LD_LIBRARY_PATH=$HOME/local/lib:$LD_LIBRARY_PATH
```

then execute:

```sh
source $HOME/.bash_profile
```

to activate these changes without having to start a new terminal session.
Now use:

```sh
echo $PATH
echo $LD_LIBRARY_PATH
```

to print the value of these variables.
On my Linux box, I get:

```sh
echo $PATH
/home/gmatteo/local/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin

echo $LD_LIBRARY_PATH
/home/gmatteo/local/lib:
```

Note how `/home/gmatteo/local/bin` has been prepended to the previous value of $PATH.
From now on, we can invoke any executable in $HOME/local/bin by just typing its base name in the shell
without having to the type the full path.

Using:

```sh
export PATH=$HOME/local/bin
```

is not a very good idea as the shell will stop working. Can you explain why?

!!! tip

    MaxOsx users should replace `LD_LIBRARY_PATH` with `DYLD_LIBRARY_PATH`

    Remember also that one can use `env` to print all the environment variables defined
    in your session and pipe the results to other Unix tools.
    Try e.g.:

    ```sh
    env | grep LD_
    ```

    to print only the variables whose name starts with **LD_**


## How to compile libxc

At this point, it should not be so difficult to compile and install the *libxc* library that provides
many useful XC functionals such as Meta-GGA and hybrid functionals.
Libxc is written in C and can be built using the standard `configure && make` approach.
No external dependency is needed, except for basic C libraries that are available
on any decent Linux distribution.

Let's start by fetching the tarball from the internet:

```sh
# Get the tarball.
# Mind the -O option used in wget to specify the name of the output file

cd $HOME/local/src
wget http://www.tddft.org/programs/libxc/down.php?file=4.3.4/libxc-4.3.4.tar.gz -O libxc.tar.gz
tar -zxvf libxc.tar.gz
```

Now configure the package with the `--prefix` option,
to specify the location where all the libraries, executables, include files,
Fortran modules, man pages, etc will be installed when we execute `make install`.

```sh
cd libxc-4.3.4 && ./configure --prefix=$HOME/local
```

Finally, build the library, run the tests and install libxc with:

```sh
make -j2
make check && make install
```

At this point, we should have the following libraries installed in ~/local/lib

```sh
ls ~/local/lib/libxc*
/home/gmatteo/local/lib/libxc.a   /home/gmatteo/local/lib/libxcf03.a   /home/gmatteo/local/lib/libxcf90.a
/home/gmatteo/local/lib/libxc.la  /home/gmatteo/local/lib/libxcf03.la  /home/gmatteo/local/lib/libxcf90.la
```

where:

  * **libxc** is the C library
  * **libxcf90** is the library with the F90 API
  * **libxcf03** is the library with the F2003 API


Both *libxcf90* and *libxcf03* depends on the C library where most of the work is done.
At present, ABINIT requires the F90 API only so one should pass the compiler

    -L$HOME/local/lib -lxcf90 -lxc

for the libraries and

    -I$HOME/local/include

for the include files.

Note how `libxcf90` comes **before** the C library `libxc`.
This is done on purpose as `libxcf90` depends on `libxc` (the Fortran API calls the C implementation).
Inverting the order of the libraries will likely trigger errors (**undefined references**)
in the last step of the compilation when the linker tries to build the final application.

!!! important

    Things become more complicated when we have to build applications using many different interdependent
    libraries as the order of the libraries passed to the linker is of crucial importance.
    Fortunately the ABINIT build system is aware of this problem and all the dependencies
    (BLAS, LAPACK, FFT, LIBXC, MPI, etc) will be automatically put in the right order so
    you don't have to worry about this point although it is worth knowing about it.

Since this is the first time we use a library with Fortran bindings, it is worth to clarify
that the bindings must be built with the **same compiler as the one we will use to compile Abinit**.
In a nutshell, Fortran/C++ libraries are compiler-dependent because these two languages are relatively high-level
so they need to rely on implementation details.
C libraries, on the other hand, are usually more portable as the C language has less abstractions
and, last but not least, the Linux-OS is written in C, the Linux kernel
is usually compiled with GCC and all the other C compilers need to maintain binary compatibility with the GCC ABI.
If you wonder about the difference between API and ABI, please read this
[stackoverflow post](https://stackoverflow.com/questions/2171177/what-is-an-application-binary-interface-abi).

!!! tip

    In principle, one can reuse libraries as long as the major version of the Fortran compiler is the same
    but experience has shown that it's always a good idea to require strict matching.

<!--
At present, the Fortran interface is not required by Abinit, only the C library.
We made this choice, because one can easily use the C library with Abinit compiled with different Fortran compilers/version
without having to rebuild libxc with a different Fortran compiler.
Note, however, that for other libraries (fftw3, netcdf) we will employ Fortran bindings.

TODO: Discuss FCFLAGS and LDFLAGS
https://www.gnu.org/software/make/manual/html_node/Implicit-Variables.html
-->

TODO: FCFLAGS

```md
Some influential environment variables:
  CC          C compiler command
  CFLAGS      C compiler flags
  LDFLAGS     linker flags, e.g. -L<lib dir> if you have libraries in a
              nonstandard directory <lib dir>
  LIBS        libraries to pass to the linker, e.g. -l<library>
  CPPFLAGS    (Objective) C/C++ preprocessor flags, e.g. -I<include dir> if
              you have headers in a nonstandard directory <include dir>
  CPP         C preprocessor
  CXX         C++ compiler command
  CXXFLAGS    C++ compiler flags
  FC          Fortran compiler command
  FCFLAGS     Fortran compiler flags
```

## Compiling and installing FFTW

FFTW is a C library for computing the Fast Fourier transform in one or more dimensions.
ABINIT already provides an internal implementation of the FFT algorithm implemented in Fortran
hence FFTW is considered an optional dependency.
Nevertheless, **we do not recommend the internal implementation if you really care about performance**.
The reason is that FFTW (or, even better, the DFTI library provided by intel MKL)
is usually much faster than the internal version.

!!! important

    FFTW is very easy to install on Linux machines once you have *gcc* and *gfortran*.
    The [[fftalg]] variable defines the implementation to be used and 312 corresponds to the FFTW implementation.
    The default value of [[fftalg]] is automatically set by the *configure* script via pre-preprocessing options.
    In other words, if you activate support for FFTW (DFTI) at configure time,
    ABINIT will use [[fftalg]] 312 (512) as default.

The FFTW source code can be downloaded from [fftw.org](http://www.fftw.org/),
and the tarball of the latest version is available at <http://www.fftw.org/fftw-3.3.8.tar.gz>.

```sh
cd $HOME/local/src
wget http://www.fftw.org/fftw-3.3.8.tar.gz
tar -zxvf fftw-3.3.8.tar.gz && cd fftw-3.3.8
```

The compilation procedure is very similar to the one already used for the libxc package.
Note, however, that ABINIT needs both the **single-precision** and the **double-precision** version.
This means that we need to configure, build and install the fftw package **twice**.

To build the single precision version, use:

```sh
./configure --prefix=$HOME/local --enable-single
make -j2
make check && make install
```

Let's have a look at the libraries we've just installed with:

```sh
ls $HOME/local/lib/libfftw3*
/home/gmatteo/local/lib/libfftw3f.a  /home/gmatteo/local/lib/libfftw3f.la
```

the `f` at the end stands for `float` (C jargon for single precision).

Now we configure for the double precision version (default behaviour)

```sh
./configure --prefix=$HOME/local
make -j2
make check && make install
```

After this step, you should have both the single (`f`) and the double (`l`) version of the library:

```sh
ls $HOME/local/lib/libfftw3*
/home/gmatteo/local/lib/libfftw3.a   /home/gmatteo/local/lib/libfftw3f.a
/home/gmatteo/local/lib/libfftw3.la  /home/gmatteo/local/lib/libfftw3f.la
```

To compile ABINIT with FFTW3 support, one should use:

    -L$HOME/local/lib -lfftw3f -lfftw3 -I$HOME/local/include

Note that, unlike in *libxc*, here we don't have to specify different libs for Fortran and C
as the FFTW3 developers decided to bundle both the C and the Fortran API in the same library.
The Fortran interface is included by default provided the FFTW3 configure script can find a Fortran compiler on your system.
In our case, we know that our FFTW3 libraries support Fortran as *gfortran* in available
but this may not be true if we are using a precompiled library installed with your package manager.

To make sure we have the Fortran API bundled in the library, one can use the `nm` tool
to get the list of symbols in the library and then use *grep* to search for the Fortran API.
For instance we can check whether the library contains the Fortran routine for multiple single-precision
FFTs (*sfftw_plan_many_dft*) and the version for multiple double-precision FFTs (*dfftw_plan_many_dft*)

```sh
[gmatteo@bob fftw-3.3.8]$ nm $HOME/local/lib/libfftw3f.a | grep sfftw_plan_many_dft
0000000000000400 T sfftw_plan_many_dft_
0000000000003570 T sfftw_plan_many_dft__
0000000000001a90 T sfftw_plan_many_dft_c2r_
0000000000004c00 T sfftw_plan_many_dft_c2r__
0000000000000f60 T sfftw_plan_many_dft_r2c_
00000000000040d0 T sfftw_plan_many_dft_r2c__

[gmatteo@bob fftw-3.3.8]$ nm $HOME/local/lib/libfftw3.a | grep dfftw_plan_many_dft
0000000000000400 T dfftw_plan_many_dft_
0000000000003570 T dfftw_plan_many_dft__
0000000000001a90 T dfftw_plan_many_dft_c2r_
0000000000004c00 T dfftw_plan_many_dft_c2r__
0000000000000f60 T dfftw_plan_many_dft_r2c_
00000000000040d0 T dfftw_plan_many_dft_r2c__
```

If you are using a FFTW3 library without Fortran API, the ABINIT configure script will complain that the library
cannot be called from Fortran and you will need to dig into *config.log* to understand what's going on.

!!! note

    At present, there is no need to compile FFTW with MPI support because ABINIT implements its own
    version of the MPI-FFT algorithm based on the sequential FFTW version.
    The MPI algorithm implemented in ABINIT is optimized for plane-waves codes
    as it supports zero-padding and composite transforms for the applications of the local part of the KS potential.


## Installing MPI

In this section, we discuss how to compile and install the MPI library.
This step is required if you want to run ABINIT with multiple processes and/or you
need to compile MPI-based libraries such as PBLAS/Scalapack or the HDF5 library with support for parallel IO.

It is worth stressing that the MPI installation provides two scripts (**mpif90** and **mpicc**)
that act as a sort of wrapper around the sequential Fortran and the C compilers, respectively.
These scripts **must be used** to compile parallel software using MPI instead
of the "sequential" *gfortran* and *gcc*.
The MPI library also provides launcher scripts installed in the *bin* directory (*mpirun* or *mpiexec*)
that must be used to execute MPI applications with NUM_PROCS MPI processes with the syntax:

```sh
mpirun -n NUM_PROCS EXECUTABLE [EXEC_ARGS]
```

!!! warning

    Keep in mind that there are **several MPI implementations** available around
    (*openmpi*, *mpich*, *intel mpi*, etc) and you must **choose one implementation and stick to it**
    when building your software stack.
    In other words, all the libraries and executables requiring MPI must be compiled, linked and executed
    **with the same MPI library**.

    Don't try to link a library compiled with e.g. *mpich* if you are building the code with
    the *mpif90* wrapper provided by e.g. *openmpi*.
    By the same token, don't try to run executables compiled with e.g. *intel mpi* with the
    *mpirun* launcher provided by *openmpi* unless you are looking for troubles!
    Again, the *which* command is quite useful to pinpoint possible problems especially if there are multiple
    installations of MPI in your $PATH (not a very good idea!).

In this tutorial, we employ the *mpich* implementation that can be downloaded
from this [webpage](https://www.mpich.org/downloads/).
In the terminal, issue:

```sh
cd $HOME/local/src
wget http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz
tar -zxvf mpich-3.3.2.tar.gz
cd mpich-3.3.2/
```

to download and uncompress the tarball.
Then configure/compile/test/install the library with:

```sh
./configure --prefix=$HOME/local
make -j2
make check && make install
```

Once the installation is completed, we should obtain this message.

```sh
----------------------------------------------------------------------
Libraries have been installed in:
   /home/gmatteo/local/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the '-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable
     during execution
   - add LIBDIR to the 'LD_RUN_PATH' environment variable
     during linking
   - use the '-Wl,-rpath -Wl,LIBDIR' linker flag
   - have your system administrator add LIBDIR to '/etc/ld.so.conf'

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------
```

The reason why one should add `$HOME/local/lib` to `$LD_LIBRARY_PATH` now should be clear to you.

Let's have a look at the MPI executables we have just installed in $HOME/local/bin:

```sh
ls $HOME/local/bin/mpi*
/home/gmatteo/local/bin/mpic++        /home/gmatteo/local/bin/mpiexec        /home/gmatteo/local/bin/mpifort
/home/gmatteo/local/bin/mpicc         /home/gmatteo/local/bin/mpiexec.hydra  /home/gmatteo/local/bin/mpirun
/home/gmatteo/local/bin/mpichversion  /home/gmatteo/local/bin/mpif77         /home/gmatteo/local/bin/mpivars
/home/gmatteo/local/bin/mpicxx        /home/gmatteo/local/bin/mpif90
```

Since we added $HOME/local/bin to our $PATH, we should see that *mpi90* is actually
pointing to the version we have just installed:

```sh
which mpif90
~/local/bin/mpif90
```

As already mentioned, *mpif90* is a wrapper around the sequential Fortran compiler.
To show the Fortran compiler invoked by *mpif90*, use:

```sh
mpif90 -v

mpifort for MPICH version 3.3.2
Using built-in specs.
COLLECT_GCC=gfortran
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/5.3.1/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,objc,obj-c++,fortran,ada,go,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --disable-libgcj --with-isl --enable-libmpx --enable-gnu-indirect-function --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
gcc version 5.3.1 20160406 (Red Hat 5.3.1-6) (GCC)
```

The C include files (*.h*) and the Fortran modules (*.mod*) have been installed
in our $HOME/local/include directory:

```sh
ls $HOME/local/include/mpi*

/home/gmatteo/local/include/mpi.h              /home/gmatteo/local/include/mpicxx.h
/home/gmatteo/local/include/mpi.mod            /home/gmatteo/local/include/mpif.h
/home/gmatteo/local/include/mpi_base.mod       /home/gmatteo/local/include/mpio.h
/home/gmatteo/local/include/mpi_constants.mod  /home/gmatteo/local/include/mpiof.h
/home/gmatteo/local/include/mpi_sizeofs.mod
```

In principle, the location of the directory must be passed to the Fortran compiler either
with the `-J` (for MPI2+) or the `-I` option (for MPI1).
Fortunately, the ABINIT build system can automatically detect your MPI installation and set all the compilation
options automatically if you provide the installation root ($HOME/local).

!!! warning

    The `.mod` files are **compiler- and version-dependent**.
    In other words, one cannot use these `.mod` files to compile code with a different Fortran compiler.
    Moreover, you should not expect to be able to use modules compiled with
    a different version of the different compiler.
    This is one of the reasons why the version of the Fortran compiler used
    to build the software stack is very important.

## Installing HDF5 and netcdf4

In Abinit9, HDF5 and netcdf4 have become **hard-requirements**.
This means that these two libraries are now mandatory.
The reason is that the Abinit developers are moving away from Fortran binary files as this
format is not portable and difficult to read from high-level languages such as python.

Netcdf4 is built on top of HDF5 and consists of two different layers:

* The **low-level** C library

* The **Fortran bindings** i.e. Fortran routines calling the low-level C implementation.
  This is the high-level API/ABI used by ABINIT to perform all the IO operations on netcdf files.

<!--
  If you wonder about the difference between API and ABI, please read this
  [stackoverflow post](https://stackoverflow.com/questions/2171177/what-is-an-application-binary-interface-abi).
  Again this is one of the reasons why you should compile all your software stack with the same compiler
  and possibly with the same (major) version.
-->

To build the libraries required by ABINIT, we will compile the different layers
in a bottom-up fashion starting from the HDF5 package.
Since we want to activate support for parallel IO, we need to compile the library using the wrappers
provided by our MPI installation instead of using *gcc* or *gfortran* directly.

Let's start by downloading the HDF5 tarball from this [download page](https://www.hdfgroup.org/downloads/hdf5/source-code/).
Uncompress the archive with *tar* as usual, then configure the package with:

```sh
./configure --prefix=$HOME/local/ \
            CC=$HOME/local/bin/mpicc --enable-parallel --enable-shared
```

where we've used the *CC* variable to specify the C compiler.
This step is crucial in order to activate support for parallel IO.

At the end of the configuration step, you should get the following output:

```sh
                     AM C Flags:
               Shared C Library: yes
               Static C Library: yes


                        Fortran: no

                            C++: no

                           Java: no


Features:
---------
                   Parallel HDF5: yes
Parallel Filtered Dataset Writes: yes
              Large Parallel I/O: yes
              High-level library: yes
                    Threadsafety: no
             Default API mapping: v110
  With deprecated public symbols: yes
          I/O filters (external): deflate(zlib)
                             MPE:
                      Direct VFD: no
                         dmalloc: no
  Packages w/ extra debug output: none
                     API tracing: no
            Using memory checker: no
 Memory allocation sanity checks: no
          Function stack tracing: no
       Strict file format checks: no
    Optimization instrumentation: no
```

The line with:

```sh
Parallel HDF5: yes
```

tells us that our HDF5 build supports parallel IO.
Also note that, as far as ABINIT is concerned, Fortran support is **optional**
as ABINIT will be interfaced with HDF5 through the Fortran bindings provided by netcdf-fortran.
In other words, ABINIT requires netcdf-fortran and not the HDF5 Fortran bindings.

Again, issue `make -j NUM` followed by `make check` (**it may take some time!**) and finally `make install`.

Now let's move to netcdf.
Download the C version and the Fortran bindings from the
[netcdf website](https://www.unidata.ucar.edu/downloads/netcdf/) with:

```sh
wget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-c-4.7.3.tar.gz
wget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-fortran-4.5.2.tar.gz
```

and unpack the tarball files as usual.

To compile the C library, use:

```sh
cd netcdf-c-4.7.3
./configure --prefix=$HOME/local/ \
            CC=$HOME/local/bin/mpicc \
            LDFLAGS=-L$HOME/local/lib CPPFLAGS=-I$HOME/local/include
```

where, again, we use `mpicc` as C compiler.
At the end of the configuration step, one should obtain

```sh
# NetCDF C Configuration Summary
==============================

# General
-------
NetCDF Version:		4.7.3
Dispatch Version:       1
Configured On:		Wed Apr  8 00:53:19 CEST 2020
Host System:		x86_64-pc-linux-gnu
Build Directory: 	/home/gmatteo/local/src/netcdf-c-4.7.3
Install Prefix:         /home/gmatteo/local

# Compiling Options
-----------------
C Compiler:		/home/gmatteo/local/bin/mpicc
CFLAGS:
CPPFLAGS:		-I/home/gmatteo/local/include
LDFLAGS:		-L/home/gmatteo/local/lib
AM_CFLAGS:
AM_CPPFLAGS:
AM_LDFLAGS:
Shared Library:		yes
Static Library:		yes
Extra libraries:	-lhdf5_hl -lhdf5 -lm -ldl -lz -lcurl

# Features
--------
NetCDF-2 API:		yes
HDF4 Support:		no
HDF5 Support:		yes
NetCDF-4 API:		yes
NC-4 Parallel Support:	yes
PnetCDF Support:	no
DAP2 Support:		yes
DAP4 Support:		yes
Byte-Range Support:	no
Diskless Support:	yes
MMap Support:		no
JNA Support:		no
CDF5 Support:		yes
ERANGE Fill Support:	no
Relaxed Boundary Check:	yes
```

Now use the standard sequence of commands to compile and install the package:

```sh
make -j2
make check && make install
```

Once installation is completed, use the `nc-config` executable to
inspect the features provided by the library we've just installed.

```sh
[gmatteo@bob netcdf-c-4.7.3]$ which nc-config
~/local/bin/nc-config
```

To get a summary of the options used to build the C layer and the available features, use

```sh
[gmatteo@bob netcdf-c-4.7.3]$ nc-config --all

This netCDF 4.7.3 has been built with the following features:

  --cc            -> /home/gmatteo/local/bin/mpicc
  --cflags        -> -I/home/gmatteo/local/include
  --libs          -> -L/home/gmatteo/local/lib -lnetcdf
  --static        -> -lhdf5_hl -lhdf5 -lm -ldl -lz -lcurl
```

To compile the Fortran bindings, execute:

```sh
cd netcdf-fortran-4.5.2
./configure --prefix=$HOME/local/ \
            FC=$HOME/local/bin/mpif90 \
            LDFLAGS=-L$HOME/local/lib CPPFLAGS=-I$HOME/local/include
```

where **FC** points to our *mpif90* wrapper.
Then issue:

```sh
make -j2
make check && make install
```

To inspect the features activated in our Fortran library, use `nf-config` instead of **nc-config**:

```sh
which nf-config
```

To get a summary of the options used to build the Fortran bindings and the list of available features, use

```sh
[gmatteo@bob netcdf-c-4.7.3]$ nf-config --all
```

See also <https://www.unidata.ucar.edu/software/netcdf/docs/building_netcdf_fortran.html>

## How to compile ABINIT

In this section, we finally discuss how to compile ABINIT using the
MPI compilers and the libraries installed previously.
First of all, download the ABINIT tarball from [this page](https://www.abinit.org/packages) using

```sh
wget https://www.abinit.org/sites/default/files/packages/abinit-9.0.2.tar.gz
```

In this case, we are using version 9.0.2 but you may want to download the
latest production version to take advantage of new features and benefit from bug fixes.
Once you got the tarball, uncompress it by typing:

```sh
tar -xvzf abinit-9.0.2.tar.gz
```

Then `cd` into the newly created *abinit-9.0.2* directory.
Before actually starting the compilation, type:

```sh
./configure --help
```

and take some time to read the documentation of the different options.

Besides the standard environment variables: CC, CFLAGS, FC, FCFLAGS etc.
the build system provides specialized options to pass
to activate support for external libraries.
For *libxc*, for instance, we have:

```md
LIBXC_CPPFLAGS
            C preprocessing flags for LibXC.
LIBXC_CFLAGS
            C flags for LibXC.
LIBXC_FCFLAGS
            Fortran flags for LibXC.
LIBXC_LDFLAGS
            Linker flags for LibXC.
LIBXC_LIBS
            Library flags for LibXC.
```

According to what we seen during the compilation of *libxc*, one should pass to
*configure* the following options:

```sh
LIBXC_LIBS="-L$HOME/local/lib -lxcf90 -lxc"
LIBXC_FCFLAGS="-I$HOME/local/include"
```

Alternatively, one can use the **high-level interface** provided by the `--with-library`
to specify the installation directory for the library as in:

```sh
--with-libxc="$HOME/local/lib"
```

In this case, *configure* will try to automatically detect the other options.
This is the easiest approach but if *configure* cannot detect the dependency properly,
you may need to set the options manually.

<!--
https://www.cprogramming.com/tutorial/shared-libraries-linux-gcc.html
In this example, we will be taking advantage of the high-level interface provided by the *with_XXX* options
to tell the build system where external dependencies are located instead of passing options explicitly.
-->

Instead of passing options to configure from the command line, we will be using an **external file**
(*myconf.ac9*) to collect all our options.
To make *configure* read options from file use the syntax:

```sh
./configure --with-config-file="myconf.ac9"
```

where double quotes may be needed for portability reasons.

!!! important

    The name of the options in `myconf.ac9` is in **normalized form** that is
    the initial `--` is removed from the option name and all the other `-` characters in the string
    are replaced by an underscore `_`.
    Following these simple rules, the  *configure* option `--with-mpi` becomes `with_mpi` in the ac9 file.

    Also note that in the configuration file it is possible to use **shell variables**
    and reuse the output of external tools using
    [backtick syntax](https://unix.stackexchange.com/questions/48392/understanding-backtick/48393)
    as is *\`nf-config --flibs`* to reduce the amount of typing
    and have a configuration file that can be easily reused in other contexts.


```sh
# -------------------------------------------------------------------------- #
# MPI support                                                                #
# -------------------------------------------------------------------------- #

# Determine whether to build parallel code (default is auto)

# Note:
#
#   * the build system expects to find subdirectories named bin/, lib/,
#     include/ under the prefix.
#
with_mpi=$HOME/local/

# Flavor of linear algebra libraries to use (default is netlib)
#
with_linalg_flavor="openblas"

# Library flags for linear algebra (default is unset)
#
LINALG_LIBS="-L$HOME/local/lib -lopenblas"

# -------------------------------------------------------------------------- #
# Optimized FFT support                                                      #
# -------------------------------------------------------------------------- #

# Flavor of FFT framework to support (default is auto)
#
with_fft_flavor="fftw3"

FFTW3_LIBS="-L$HOME/local/lib -lfftw3f -lfftw3"

# LibXC
# Website: http://www.tddft.org/programs/octopus/wiki/index.php/Libxc

# Trigger and install prefix for LibXC (default is unset)
#
#with_libxc="$HOME/local/lib"
LIBXC_LIBS="-L$HOME/local/lib -lxcf90 -lxc"
LIBXC_FCFLAGS="-I$HOME/local/include"

# NetCDF
# Website: http://www.unidata.ucar.edu/software/netcdf/

# Trigger and install prefix for NetCDF (default is unset)
#
#with_netcdf=$(nc-config --prefix)
with_netcdf="yes"

#with_netcdf_fortran=$(nf-config --prefix)
NETCDF_FORTRAN_LIBS=`nf-config --flibs`
NETCDF_FORTRAN_FCFLAGS=`nf-config --fflags`

HDF5_LIBS=`nf-config --flibs`
HDF5_FCFLAGS=`nf-config --fflags`

# Enable OpenMP (gmatteo, torrent)
enable_openmp="no"
```

You might then find useful to have a look at other examples configuration files
available [in this page](../developers/autoconf_examples).
Other examples for compiling on clusters can be found in the
[abiconfig package](https://github.com/abinit/abiconfig).

In the previous examples, we executed *configure* in the top level directory of the package but
for ABINIT we prefer to a use a **build directory** to keep the object files
and executables separated from the source code.
Moreover, thanks to this approach, it is possible to build different versions of ABINIT using the same source
tree. For example, one can have a version compiled with *gfortran* and another build directory in which
the intel *ifort* compiler is used.

```sh
mkdir build_gfortran && cd _build_gfortran
../configure --with-config-file="myconf.ac9"
```

If everything goes as expected you should obtain the following summary
at the end of the configuration:

```sh
FOOBAR
```

Remember to always crosscheck the summary produced by the *configure* script to make sure
you are getting what you expect.

The *configure* script has generated several Makefiles required by make and the **config.h** include file
with all the pre-processing options that will be used to build ABINIT.
This file is included in every ABINIT source file and it defines the features that will be activated/deactivated
at compilation-time depending on the libraries available on your machine.
Let's have a look at a selected portion of **config.h**:

```cpp
/* Define to 1 if you have a working MPI installation. */
#define HAVE_MPI 1

/* Define to 1 if you have a MPI-1 implementation (obsolete, broken). */
/* #undef HAVE_MPI1 */

/* Define to 1 if you have a MPI-2 implementation. */
#define HAVE_MPI2 1

/* Define to 1 if you want MPI I/O support. */
#define HAVE_MPI_IO 1
```

This file tells us that we are building Abinit with MPI support,
that we have a library implementing the MPI2 specifications
and that our implementation supports parallel MPI-IO.
Of course, end users are mainly concerned with the final summary reported
by the *configure* script to understand whether
a particular feature has been activated or not but more advanced users may
find the content of `config.h` valuable to understand what's going on.

At this point, we can start to compile the package with e.g. *make -j2*.
If everything goes as expect, you should end up with a bunch of executables in *src/98_main*.
However, the fact that the compilation completed successfully does not necessarily imply that the executables
will work several things that can go wrong at runtime.

First of all, let's try to execute

```sh
abinit --version
```

If this is a parallel build, you may need to use

```sh
mpirun -n 1 abinit --version
```

even for a sequential run as certain MPI libraries are not able to bootstrap the MPI library
without *mpirun* (*mpiexec*).

To get the summary of options activated during the build, run *abinit* with the `-b` option
(or `--build` if you prefer the verbose version)

```sh
./src/98_main/abinit -b
```

If the executable does not crash, you may want to execute

```sh
make test_fast
```

to run some basic tests.
If something goes wrong, checkout the Troubleshooting section for possible solutions.

Finally you may want to execute the *runtests.py* python script in the *tests*
to validate the build before running production calculations.

```sh
cd tests
../../tests/runtests.py v1 -j4
```

As usual, use:

```sh
../../tests/runtests.py --help
```

to list the available options.
A more detailed discussion is given in [this page](testsuite_howto).

[![asciicast](https://asciinema.org/a/40324.png)](https://asciinema.org/a/40324)


### Dynamic libraries and ldd

Since we decided to compile with dynamic linking, the external libraries are not included in the final executables.
Actually, the libraries will be loaded by the OS at runtime when we start to run the executable and
the list of directories specified in `$LD_LIBRARY_PATH` (`$DYLD_LIBRARY_PATH` for MacOs) will play a crucial role.

On Linux one can use the *ldd* tool to prints the shared objects (shared libraries) required by each
program or shared object specified on the command line.

```sh
ldd abinit

FOO
```

!!! tip

    On MacOsX, replace *ldd* with *otool* and the syntax:

    ```sh
    otool -L abinit
    ```

To understand why LD_LIBRARY_PATH is so important, let's try to reset the value of this variable with

```sh
unset LD_LIBRARY_PATH
echo $LD_LIBRARY_PATH
```

then rerun *ldd* (or *otool*) again.
Do you understand what's happening here?
Can you execute *abinit* with an empty $LD_LIBRARY_PATH?
How would you fix the problem?

### Trouble shooting

Problems can appear at different levels:

* configuration time
* compilation time
* runtime *i.e.* when executing the code

**Configuration-time errors** are usually due to misconfiguration of your environment, missing (hard) dependencies
or problems in the software stack that will make *configure* abort.
Unfortunately, the error message reported by *configure* is not always self-explanatory.
To pinpoint the source of the problem you will need to inspect *config.log* for clues,
especially the error messages associated to the feature/library that is triggering the error.

This is not as easy as it looks since *configure* sometimes performs multiple tests to detect your architecture
and some of these tests are **supposed to fail**.
As a consequence, not all the error messages reported in *config.log* are necessarily relevant.
Even if you find the test that makes *configure* abort, the error message may be obscure and difficult to decipher.
In this case, you can ask for help on the forum but remember to provide enough info on your architecture, the
compilation options and, most Importantly, **a copy of _config.log_**.
Without this file, indeed, it is almost impossible to understand what's going on.

An example will help.
Let's assume we are compiling on a cluster using modules provided by our sysadmin.
More specifically, there is an `openmpi_intel2013_sp1.1.106` module that is supposed to provide
the openmpi implementation of the MPI library compiled with a particular version of the intel compiler
(remember what we said about using the same version of the compiler).
Obviously **we need to load the modules before running configure** in order to setup our environment
so we issue:

```sh
module load openmpi_intel2013_sp1.1.106
```

The module seems to work as no error message is printed to the terminal and `which mpicc` shows
that the compiler has been added to $PATH.
At this point we try to configure ABINIT with:

```sh
with_mpi_prefix="${MPI_HOME}"
```

where `$MPI_HOME` is a environment variable set by *module load* (use e.g. `env | grep MPI`).
Unfortunately, the *configure* script aborts at the very beginning complaining
that the C compiler does not work!

```md
checking for gcc... /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc
checking for C compiler default output file name...
configure: error: in `/home/gmatteo/abinit/build':
configure: error: C compiler cannot create executables
See `config.log' for more details.
```

Let's analyze the output of *configure* in more details.
The line:

```md
checking for gcc... /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc
```

indicates that *configure* was able to find *mpicc* in *${MPI_HOME}/bin*.
Then an internal test is executed to make sure the wrapper can compile a rather simple Fortran program using MPI
but the test fails and *configure* aborts immediately with the pretty explanatory message:

```md
configure: error: C compiler cannot create executables
See `config.log' for more details.
```

If we want to understand why *configure* failed, we have to **open _config.log_ in the editor**
and search for error messages towards the end of the log file.
For example one can search for the string "C compiler cannot create executables".
Immediately above this line, we find the following section:

```sh
configure:12104: checking whether the C compiler works
configure:12126: /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc conftest.c  >&5
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_reg_xrc_rcv_qp@IBVERBS_1.1'
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_modify_xrc_rcv_qp@IBVERBS_1.1'
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_open_xrc_domain@IBVERBS_1.1'
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_unreg_xrc_rcv_qp@IBVERBS_1.1'
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_query_xrc_rcv_qp@IBVERBS_1.1'
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_create_xrc_rcv_qp@IBVERBS_1.1'
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_create_xrc_srq@IBVERBS_1.1'
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_close_xrc_domain@IBVERBS_1.1'
configure:12130: $? = 1
configure:12168: result: no
configure: failed program was:
| /* confdefs.h */
| #define PACKAGE_NAME "ABINIT"
| #define PACKAGE_TARNAME "abinit"
| #define PACKAGE_VERSION "9.1.2"
| #define PACKAGE_STRING "ABINIT 9.1.2"
| #define PACKAGE_BUGREPORT "https://bugs.launchpad.net/abinit/"
| #define PACKAGE_URL ""
| #define PACKAGE "abinit"
| #define VERSION "9.1.2"
| #define ABINIT_VERSION "9.1.2"
| #define ABINIT_VERSION_MAJOR "9"
| #define ABINIT_VERSION_MINOR "1"
| #define ABINIT_VERSION_MICRO "2"
| #define ABINIT_VERSION_BUILD "20200824"
| #define ABINIT_VERSION_BASE "9.1"
| #define HAVE_OS_LINUX 1
| /* end confdefs.h.  */
|
| int
| main ()
| {
|
|   ;
|   return 0;
| }
```

The line

```sh
configure:12126: /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc conftest.c  >&5
```

tells us that *configure* tried to compile a C file named *conftest.c* and that the return value
stored in the `$?` shell variable is non-zero thus indicating failure:

```sh
configure:12130: $? = 1
configure:12168: result: no
```

The failing program (the C main after the line "configure: failed program was:")
is a rather simple piece of code and our *mpicc* compiler is not able to compile it!
If we look more carefully at the lines after the invocation of *mpicc*,
we see lots of undefined references to functions of the *libibverbs* library:

```sh
configure:12126: /cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/bin/mpicc conftest.c  >&5
/cm/shared/apps/openmpi/1.7.5/intel2013_sp1.1.106/lib/libmpi.so: undefined reference to `ibv_reg_xrc_rcv_qp@IBVERBS_1.1
```

This looks like some mess in the system configuration and not necessarily a problem in the ABINIT build system.
Perhaps there have been changes to the environment, maybe a system upgrade or the module is simply broken.
In this case you should send the *config.log* to the sysadmin so that he/she can fix the problem or just use
another more recent module.

Obviously, one can encounter cases in which modules are properly configured yet the *configure* script aborts
because it does not know how to deal with your software stack.
In both cases, *config.log* is key to pinpoint the problem and sometimes you will find that
the problem is rather simple to solve.
For example you are trying to use F90 module files produced by *gfortran* while trying to compile with the
intel compiler or perhaps you are trying to use modules produced by a different version of the same compiler!.
Perhaps you forgot to add the include directory required by an external library and the compiler
cannot find the include file or maybe there is a typo in the configuration options.
The take home message is that several mistakes can be detected by just **inspecting the log messages**
reported in *configure.log* if you know how to search for them.

**Compilation-time errors** are usually due to syntax errors, portability issues or
Fortran constructs that are not supported by that particular version of the compiler.
In the first two cases, please report the problem on the forum.
In the later case, you will need a more recent version of the compiler.
Sometimes the compilation aborts with an **internal compiler error** that should be considered
as a bug in the compiler rather than a errors in the ABINIT source code.
Decreasing the optimization level when compiling the particular routine that triggers the error
(use -O1 or even -O0 for the most problematic cases) may solve the problem else
try a more recent version of the compiler.
It is also possible to trigger internal compiler errors when trying to use modules
produced by different versions of the same compiler.
In this case, the solution is straightforward: use libraries and modules built with the same version.

**Runtime errors** are more difficult to fix as they may require the use of a debugger or some basic
understanding of Linux signals.
If the code raises the **SIGILL** signal, it means that the CPU attempted to execute
an instruction it didn't understand.
Very likely, your executables/libraries have been compiled for the **wrong architecture**.
This may happen on clusters when the CPU family available on the frontend differs
from the one available on the compute node
and aggressive optimization options are used (-O3, --march, -xHost etc).
Removing the optimization options and using the much safer -O2 level may help.
Alternatively, one can compile the source directly on the compute node or use compilation options
compatible both with the frontend and the compute node (ask your sysadmin for details).
Feel free to complain to your sysadmin if your cluster consists of an heterogeneous system
of compute nodes as many MPI codes assume consistency at the binary level *i.e.*
the same input should produce the same output on different MPI processes,
where equality means the **same sequence of bits**.
If you are forced to work with such a weird machine, use the options provided by the resource manager
to select the compute nodes with the same CPU family.

Segmentation faults (**SIGSEGV**) are usually due to bugs in the code but they may also be
triggered by non-portable code or misconfiguration of the software stack.
When reporting this kind of problem on the forum, please add an input file so that developers
can try to reproduce the problem.
Keep in mind, however, that the problem may not be reproducible on other architectures.
The ideal solution would be to run the code under the control of the debugger,
use the backtrace to locate the line of code where the segmentation fault occurs and then
report the outcome on the forum.

Using the debugger in sequential is really simple.
First of all, make sure the code has been compiled with the `-g` option
to generate source-level debug information.
To use the GNU debugger `gdb`, perform the following operations:

1. Load the executable in the GNU debugger using the syntax:

    gdb path_to_abinit_executable

2. Run the code with the gdb *run* command and redirect the standard input with:

    (gdb) run t01.in

Wait for the error e.g. SIGSEGV, then print the **backtrace** with:

    (gdb) bt

<!--
How to use LLDB

$ lldb ../../../src/98_main/abinit
(lldb) target create "../../../src/98_main/abinit"
Current executable set to '../../../src/98_main/abinit' (x86_64).
(lldb) settings set target.input-path t85.in
(lldb) run

Avoid VECLIB on MacOsx

	/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib (compatibility version 1.0.0, current version 1.0.0)
	/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib (compatibility version 1.0.0, current version 1.0.0)

```sh
# Enable ZDOTC and ZDOTU bugfix (gmatteo)
#
enable_zdot_bugfix="yes"
```
-->

!!! tip

    Avoid debugging code compiled with `-O3` or `-Ofast` as the backtrace may not be reliable.

## How to compile ABINIT on a cluster with the intel toolchain

On intel-based clusters, we suggest to compile ABINIT with the intel compilers and the MKL library
in order to achieve better performance.
The MKL library, indeed, provides highly-optimized implementations for BLAS, LAPACK, FFT, and SCALAPACK
that can lead to a **significant speedup** while simplifying considerably the compilation process.

In what follows, we assume a cluster in which the sysadmin has already installed all the modules
(compilers, MPI and libs) required to compile ABINIT.
If some of the required libraries are lacking, it should not be that difficult to reuse the expertise acquired
in this tutorial to build and install your own libraries inside your $HOME/local directory although
the best solution would be to ask the sysadmin to provide modules with the dependencies
required by Abinit or, even better, a module that loads the ABINIT executable
and all it dependencies (feel free to ask your sysadmin to provide such a module).

For a quick introduction to the environment modules, please consult
[this documentation](https://support.ceci-hpc.be/doc/_contents/UsingSoftwareAndLibraries/UsingPreInstalledSoftware/index.html).

To list the available modules, use:

```sh
module avail
```

Hopefully, the sysadmin has already installed for you all the dependencies required by ABINIT
using a consistent tool chain i.e. same compiler version, same MPI version.
If this is not the case, ask the sysadmin to add the missing modules.
If, on the other hand, the sysadmin does not respond to your mails because is busy watching
kittens videos on youtube, you can
always compile the library from source and install it using the `make && make install`
procedure discussed in this tutorial.

```sh
module load
```

To list the modules currently loaded use:

```sh
module list
```

At this point, you should decide the toolchain to be used to compile ABINIT:
the compiler e.g. intel *ifort*, GNU *gfortran*
This part depends on the software available on the cluster.

To see what libraries are recommended for a particular use case, specify the parameters in the drop down lists below
[mkl-link-line-advisor](https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor)


## How to compile ABINIT with support for OpenMP threads

Compiling ABINIT with OpenMP is not that difficult as everything boils down to:

* Using a **threaded version** for the BLAS/LAPACK/FFT libraries
* Passing **enable_openmp="yes"** to the ABINIT configure script
  so that OpenMP is activated also at level of the ABINIT routines.

On the contrary, answering the questions:

* When and why should I use OpenMP threads for my calculations?
* How many threads should I use and what is the parallel speedup I should expect?

is much more difficult as there are several factors that should be taken into account.

To keep a long story short, one should use OpenMP threads
when we start to trigger **limitations or bottlenecks in the MPI implementation**,
especially at the level of the memory requirements or in terms of parallel scalability.
These problems are usually observed in large calculations, that is large [[natom]], [[mpw]], [[nband]].

As a matter of fact, it does not make any sense to compile ABINIT with OpenMP
if your calculations are relatively small.
Indeed, ABINIT is mainly designed with MPI-parallelism in mind.
Calculations with a relatively large number of $\kk$-points will benefit more of MPI than OpenMP,
especially if the number of MPI processes divides exactly the number of $\kk$-points.
Even worse, do not compile the code with OpenMP support if you do not plan to use threads because the OpenMP
version will have an **additional overhead** due to the creation of the threaded sections.
Remember also that increasing the number of threads does not necessarily leads to faster calculations
and the same is true for MPI.
There's always an optimal value for the number of threads beyond which the parallel efficiency start to decrease.
Unfortunately, this value is strongly hardware and software dependent so you will need to benchmark the code
before running production calculations.

After this necessary preamble, let's discuss how to compile a threaded version.
To activate OpenMP support in the Fortran routines of ABINIT, pass

```sh
enable_openmp="yes"
```

to the configure script via the configuration file.
This will automatically activate the compilation option needed to enable OpenMP support in the ABINIT source code
(`-fopenmp` for *gfortran*).
Note that this option is just part of the story as a significant fraction of the wall-time is spent in external
BLAS/FFT routines so **do not expect big speedups if you do not use threaded libraries**.

If you are building your own software stack for BLAS/LAPACK and FFT, you will need to
use the configure with the correct options for the OpenMP version and then
*make and make install* again to have the threaded version.
Also the name of libraries may change.
FFTW3, for example, ships the OpenMP version in *libfftw3_omp*
(see the [official documentation](http://www.fftw.org/fftw3_doc/Installation-and-Supported-Hardware_002fSoftware.html#Installation-and-Supported-Hardware_002fSoftware) hence **FFTW3_LIBS** should be changed accordingly.

Life is much easier if you are using intel MKL because in this case
it is just a matter of selecting *OpenMP threading* as threading layer
in the [mkl-link-line-advisor interface](https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor)
and then pass the link line and the compiler options to the ABINIT build system together with `enable_openmp="yes"`.

!!! Important

    When using threaded libraries remember to set explicitly the number of threads with e.g.

    ```sh
    export OMP_NUM_THREADS=2
    ```

    either in your *bash_profile* or in the submission script.
    By default, indeed, OpenMP uses all the cores available on the system so it's very easy to overload
    the system especially when one starts to use threads in conjunction with MPI processes.

    To run the test suite with e.g. two threads, use the `-o2` option of *runtests.py*

    We also recommend to increase the limit for the stack size using e.g.

    ```sh
    ulimit -s unlimited
    ```

    if the sysadmin allows you to do so.
